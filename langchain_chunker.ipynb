{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b78c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=400, min_chunk_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "281a8cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# INTRODUCTION *Pathfinder is a game of imagination where you can bring nearly any idea to life. This book combines with Player Core, expanding on its options and giving you even more ways to play the character you want!*\n",
      "## MORE OF EVERYTHING *Player Core 2* provides even more options for player characters. Along with the ancestries and classes summarized on the following page, there are additional general and skill feats, equipment, spells, and treasure round out your characters' abilities.\n",
      "Alongside the eight new multiclass archetypes for the classes in this book, you can find archetypes for all sorts of characters starting on page 172. These can expand any Pathfinder character in unexpected directions, helping them discover new abilities to match their developing stories. Many of these archetypes support combat specializations, like the bastion and wrestler, while others show off the roles characters may be stepping into, like the blessed one and marshal. Even characters' growing skills can uncover deeper specializations, like the archaeologist and medic.\n",
      "In addition to an expansive slate of ancestries, this book also contains three versatile heritages suitable for characters of any ancestry:\n",
      "- • Dhampirs are the mortal offspring of vampires, walking the line between life and undeath. - • Dragonbloods possess obvious draconic traits owing to dragons' influence, either direct or on their ancestors. - • Duskwalkers are souls reincarnated as a result of a bargain between psychopomps, the guides of the dead.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# File path for the JSON file\n",
    "file_path = \"data/PC 2/PC_2_combined_cleaned.json\"\n",
    "\n",
    "# Load JSON file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract the \"text\" field from each entry\n",
    "text_chunks = [entry[\"text\"] for entry in data if \"text\" in entry]\n",
    "\n",
    "# Print the first few extracted texts to verify\n",
    "print(\"\\n\".join(text_chunks[:5]))  # Displaying only the first few entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd81f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section taken from community databricks project\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import re\n",
    "\n",
    "def perform_semantic_chunking(document, chunk_size=500, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Performs semantic chunking on a document using recursive character splitting \n",
    "    at logical text boundaries.\n",
    "    \n",
    "    Args:\n",
    "        document (str): The text document to process\n",
    "        chunk_size (int): The target size of each chunk in characters\n",
    "        chunk_overlap (int): The number of characters of overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        list: The semantically chunked documents with metadata\n",
    "    \"\"\"\n",
    "    # Create the text splitter with semantic separators\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    # Split the text into semantic chunks\n",
    "    semantic_chunks = text_splitter.split_text(document)\n",
    "    print(f\"Document split into {len(semantic_chunks)} semantic chunks\")\n",
    "    \n",
    "    # Determine section titles for enhanced metadata\n",
    "    section_patterns = [\n",
    "        r'^#+\\s+(.+)$',      # Markdown headers\n",
    "        r'^.+\\n[=\\-]{2,}$',  # Underlined headers\n",
    "        r'^[A-Z\\s]+:$'       # ALL CAPS section titles\n",
    "    ]\n",
    "    \n",
    "    # Convert to Document objects with enhanced metadata\n",
    "    documents = []\n",
    "    current_section = \"Introduction\"\n",
    "    \n",
    "    for i, chunk in enumerate(semantic_chunks):\n",
    "        # Try to identify section title from chunk\n",
    "        chunk_lines = chunk.split('\\n')\n",
    "        for line in chunk_lines:\n",
    "            for pattern in section_patterns:\n",
    "                match = re.match(pattern, line, re.MULTILINE)\n",
    "                if match:\n",
    "                    current_section = match.group(0)\n",
    "                    break\n",
    "        \n",
    "        # Calculate semantic density (ratio of non-stopwords to total words)\n",
    "        words = re.findall(r'\\b\\w+\\b', chunk.lower())\n",
    "        stopwords = ['the', 'and', 'is', 'of', 'to', 'a', 'in', 'that', 'it', 'with', 'as', 'for']\n",
    "        content_words = [w for w in words if w not in stopwords]\n",
    "        semantic_density = len(content_words) / max(1, len(words))\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"chunk_id\": i,\n",
    "                \"total_chunks\": len(semantic_chunks),\n",
    "                \"chunk_size\": len(chunk),\n",
    "                \"chunk_type\": \"semantic\",\n",
    "                \"section\": current_section,\n",
    "                \"semantic_density\": round(semantic_density, 2)\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e4641d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 3769 semantic chunks\n",
      "Chunk ID: 500, Size: 116\n",
      "Section: # COMMON BACKGROUNDS *Your character's abilities don't spring into existence at the moment they take up the adventuring life. Their background—the role they had before they became an adventurer—also provides a number of abilities.*\n",
      ". You're trained in the Occultism skill and the Herbalism Lore skill. You gain the Root Magic skill feat (page 233).\n",
      "Semantic Density: 0.76\n",
      "Chunk ID: 501, Size: 454\n",
      "Section: # COMMON BACKGROUNDS *Your character's abilities don't spring into existence at the moment they take up the adventuring life. Their background—the role they had before they became an adventurer—also provides a number of abilities.*\n",
      "SABOTEUR BACKGROUND Whether you do it for personal enjoyment or at the behest of a mercenary company or military organization, you have a knack for destroying things. You have a sense for an object or structure's weak spots and know where to deliver a hammer strike or alchemical bomb. You adventure to hone your skills or complete a particular mission. Choose two attribute boosts. One must be to Strength or Dexterity, and one is a free attribute boost\n",
      "Semantic Density: 0.77\n",
      "Chunk ID: 502, Size: 202\n",
      "Section: # COMMON BACKGROUNDS *Your character's abilities don't spring into existence at the moment they take up the adventuring life. Their background—the role they had before they became an adventurer—also provides a number of abilities.*\n",
      ". One must be to Strength or Dexterity, and one is a free attribute boost. You're trained in the Thievery skill and the Engineering Lore skill. You gain the Concealing Legerdemain skill feat (page 228).\n",
      "Semantic Density: 0.74\n",
      "Chunk ID: 503, Size: 481\n",
      "Section: # COMMON BACKGROUNDS *Your character's abilities don't spring into existence at the moment they take up the adventuring life. Their background—the role they had before they became an adventurer—also provides a number of abilities.*\n",
      "SCAVENGER BACKGROUND You've made a living sorting through the things society throws away. You might have scavenged simply to survive, or plied a trade as a ragpicker, dung carter, or the like. While you've left that life behind, you still keep one eye on the ground out of habit. Choose two attribute boosts. One must be to Intelligence or Wisdom, and one is a free attribute boost. You're trained in the Survival skill and a Lore skill for the settlement you grew up scavenging in\n",
      "Semantic Density: 0.76\n",
      "Chunk ID: 504, Size: 134\n",
      "Section: # COMMON BACKGROUNDS *Your character's abilities don't spring into existence at the moment they take up the adventuring life. Their background—the role they had before they became an adventurer—also provides a number of abilities.*\n",
      ". You're trained in the Survival skill and a Lore skill for the settlement you grew up scavenging in. You gain the Forager skill feat.\n",
      "Semantic Density: 0.68\n"
     ]
    }
   ],
   "source": [
    "# Combining all the text chunks into a single document\n",
    "full_doc = \"\\n\".join(text_chunks)\n",
    "\n",
    "# Perform semantic chunking on the full document\n",
    "semantic_documents = perform_semantic_chunking(full_doc)\n",
    "\n",
    "# A little peak at the results\n",
    "index_to_peak = 500\n",
    "for doc in semantic_documents[index_to_peak:index_to_peak+5]:\n",
    "    print(f\"Chunk ID: {doc.metadata['chunk_id']}, Size: {len(doc.page_content)}\")\n",
    "    \n",
    "    # Printing the section title\n",
    "    print(f\"Section: {doc.metadata['section']}\")\n",
    "    \n",
    "    # Printing the content of the document\n",
    "    print(doc.page_content)\n",
    "    \n",
    "    # Printing the semantic density\n",
    "    print(f\"Semantic Density: {doc.metadata['semantic_density']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e81f2",
   "metadata": {},
   "source": [
    "Now that we have chunks, the next step is to create some embeddings. However, something to consider is whether or not the embeddings should contain the repetitive nature of the `Section` headers. Depending on the descriptiveness of the headers, it is permissible to do so. Unfortunately, with the output we currently have, it is not feasible. We will embed purely the chunks as they stand, without including the metadata, such as `Section`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be1b2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "\n",
    "# List notation for storing documents, metadata is stored separately for use later\n",
    "docs_for_store = [\n",
    "    Document(\n",
    "        page_content=f\"passage: {doc.page_content}\",\n",
    "        metadata={ # Dict of metadata for each document, dropped chunk_type because I don't know what it does\n",
    "            \"section\": doc.metadata[\"section\"],\n",
    "            \"chunk_id\": doc.metadata[\"chunk_id\"],\n",
    "            \"chunk_size\": doc.metadata[\"chunk_size\"],\n",
    "            \"source\":   \"Player Core 2\"\n",
    "        }\n",
    "    )\n",
    "    for doc in semantic_documents\n",
    "]\n",
    "\n",
    "# Embedding step, utilizing the e5 model\n",
    "emb_fn = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True},   # E5 wants norm=True\n",
    ")\n",
    "\n",
    "# 4️⃣  Persist in Chroma (or your favourite store)\n",
    "vectordb = Chroma.from_documents(docs_for_store, emb_fn, persist_directory=\"chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa830fcc",
   "metadata": {},
   "source": [
    "This is running QA locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c1e873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Tengu have avian characteristics, such as beaks and talons, and are rarely more than 5 feet tall with hollow bones. Some Tengus also have vestigial wings.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain.chains import RetrievalQA\n",
    "# from langchain.llms import OpenAI\n",
    "# import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"My API Key Here\"\n",
    "# retrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=vectordb.as_retriever())\n",
    "# query = \"What do Tengus look like?\"\n",
    "# retrievalQA.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c703f6bf",
   "metadata": {},
   "source": [
    "Here is persisting a DB using chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b53b3cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"My API Key Here\"\n",
    "\n",
    "# In a notebook setting, we call persist to ensure embeddings are written to the disk.\n",
    "# As a script, this wouldn't be necessary.\n",
    "\n",
    "# vectordb.persist() # Persist the vector store disk\n",
    "# vectordb = None # Clear the variable to free memory\n",
    "\n",
    "# Load the persisted database from disk, and use it as normal\n",
    "vectordb = Chroma(persist_directory=\"chroma_db\", embedding_function=emb_fn)\n",
    "retrievalQA = RetrievalQA.from_llm(llm=OpenAI(), retriever=vectordb.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae2ad50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA suitable background for playing a pirate could be a sailor or criminal background, as they would have the necessary skills and experience for a life of piracy.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"If I wanted to play a pirate, what would be a suitable background?\"\n",
    "retrievalQA.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf1cbf",
   "metadata": {},
   "source": [
    "Cleanup time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687900a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectordb.delete_collection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
