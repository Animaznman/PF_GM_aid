{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list all unicode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique unicode characters found:\n",
      "U+00E7 : 'ç'\n",
      "U+00FC : 'ü'\n",
      "U+014C : 'Ō'\n",
      "U+2003 : '\\u2003'\n",
      "U+2013 : '–'\n",
      "U+2014 : '—'\n",
      "U+2019 : '’'\n",
      "U+201C : '“'\n",
      "U+201D : '”'\n",
      "U+2022 : '•'\n",
      "U+25C6 : '◆'\n",
      "U+25C7 : '◇'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Path to your JSON file\n",
    "json_path = 'data/Legends/Pathfinder Legends_1_50.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to extract all non-ASCII (unicode) characters from a string\n",
    "def extract_unicode(text):\n",
    "    return re.findall(r'[^\\x00-\\x7F]', text)\n",
    "\n",
    "# Collect all unicode characters from all text fields\n",
    "all_unicode_chars = set()\n",
    "for entry in data:\n",
    "    text = entry.get('text', '')\n",
    "    unicode_chars = extract_unicode(text)\n",
    "    all_unicode_chars.update(unicode_chars)\n",
    "\n",
    "# Print the unique unicode characters found\n",
    "print(\"Unique unicode characters found:\")\n",
    "for char in sorted(all_unicode_chars):\n",
    "    print(f\"U+{ord(char):04X} : {repr(char)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all unique chunk types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique chunk types found:\n",
      "figure\n",
      "marginalia\n",
      "table\n",
      "text\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSON file\n",
    "json_path = 'data/Legends/Pathfinder Legends_1_50.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Collect all unique chunk types\n",
    "chunk_types = set()\n",
    "for entry in data:\n",
    "    chunk_type = entry.get('chunk_type')\n",
    "    if chunk_type is not None:\n",
    "        chunk_types.add(chunk_type)\n",
    "\n",
    "# Print the unique chunk types\n",
    "print(\"Unique chunk types found:\")\n",
    "for ct in sorted(chunk_types):\n",
    "    print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings\n",
    "\n",
    "- can remove all of marginalia\n",
    "- need to parse and remove any mention of Jarvin\n",
    "- need to convert unicode\n",
    "- need to parse figure chunk type and only keep scene overview\n",
    "- need to stitch back into text or markdown for processing\n",
    "\n",
    "- need to decide where to split document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined json and fix page numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 JSON files in data/Legends\n",
      "Processing Pathfinder Legends_1_50.json...\n",
      "  Added 451 items, pages now offset by 51\n",
      "Processing Pathfinder Legends_51_100.json...\n",
      "  Added 527 items, pages now offset by 101\n",
      "Processing Pathfinder Legends_101_130.json...\n",
      "  Added 305 items, pages now offset by 131\n",
      "\n",
      "Successfully combined 3 files into data/Legends/Lengends_combined.json\n",
      "Total items: 1283\n",
      "Total pages: 131\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "directory_path = 'data/Legends'\n",
    "output_file = 'Lengends_combined.json'\n",
    "\n",
    "directory = Path(directory_path)\n",
    "\n",
    "if not directory.exists():\n",
    "   print(f\"Error: Directory '{directory_path}' does not exist\")\n",
    "\n",
    "# Find all JSON files matching the pattern\n",
    "json_files = list(directory.glob(\"Pathfinder Legends_*.json\"))\n",
    "\n",
    "if not json_files:\n",
    "   print(f\"No JSON files found matching pattern 'Pathfinder Legends_*.json' in {directory_path}\")\n",
    "\n",
    "# Sort files by the starting page number\n",
    "def extract_start_page(file_path):\n",
    "   match = re.search(r'_(\\d+)_\\d+\\.json$', file_path.name)\n",
    "   return int(match.group(1)) if match else 0\n",
    "\n",
    "json_files.sort(key=extract_start_page)\n",
    "\n",
    "combined_data = []\n",
    "current_page_offset = 1  # Start at 1 instead of 0\n",
    "\n",
    "print(f\"Found {len(json_files)} JSON files in {directory_path}\")\n",
    "\n",
    "for file_path in json_files:\n",
    "   print(f\"Processing {file_path.name}...\")\n",
    "   \n",
    "   try:\n",
    "       with open(file_path, 'r', encoding='utf-8') as f:\n",
    "           data = json.load(f)\n",
    "   except Exception as e:\n",
    "       print(f\"Error reading {file_path.name}: {e}\")\n",
    "       continue\n",
    "   \n",
    "   # Extract page range from filename\n",
    "   match = re.search(r'_(\\d+)_(\\d+)\\.json$', file_path.name)\n",
    "   if match:\n",
    "       start_page = int(match.group(1))\n",
    "       end_page = int(match.group(2))\n",
    "       original_page_count = end_page - start_page + 1\n",
    "   else:\n",
    "       # Fallback: count unique pages in the data\n",
    "       pages_in_file = set()\n",
    "       for item in data:\n",
    "           if 'grounding' in item:\n",
    "               for ground in item['grounding']:\n",
    "                   if 'page' in ground:\n",
    "                       pages_in_file.add(ground['page'])\n",
    "       original_page_count = len(pages_in_file) if pages_in_file else 1\n",
    "   \n",
    "   # Update page numbers and clean unicode in grounding data\n",
    "   for item in data:\n",
    "       # Clean unicode in text field\n",
    "       if 'text' in item and isinstance(item['text'], str):\n",
    "           # Normalize unicode and replace problematic characters\n",
    "           item['text'] = unicodedata.normalize('NFKD', item['text'])\n",
    "           item['text'] = item['text'].replace('\\u2022', '•').replace('\\u2013', '–').replace('\\u2014', '—')\n",
    "           item['text'] = item['text'].replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "           item['text'] = item['text'].replace('\\u201c', '\"').replace('\\u201d', '\"')\n",
    "           item['text'] = item['text'].replace('\\u2026', '...').replace('\\u00a0', ' ')\n",
    "       \n",
    "       if 'grounding' in item:\n",
    "           for grounding in item['grounding']:\n",
    "               if 'page' in grounding:\n",
    "                   grounding['page'] = grounding['page'] + current_page_offset\n",
    "   \n",
    "   # Add to combined data\n",
    "   combined_data.extend(data)\n",
    "   \n",
    "   # Update offset for next file\n",
    "   current_page_offset += original_page_count\n",
    "   print(f\"  Added {len(data)} items, pages now offset by {current_page_offset}\")\n",
    "\n",
    "# Write combined file to the same directory\n",
    "output_path = directory / output_file\n",
    "try:\n",
    "   with open(output_path, 'w', encoding='utf-8') as f:\n",
    "       json.dump(combined_data, f, indent=2, ensure_ascii=False)\n",
    "   \n",
    "   print(f\"\\nSuccessfully combined {len(json_files)} files into {output_path}\")\n",
    "   print(f\"Total items: {len(combined_data)}\")\n",
    "   print(f\"Total pages: {current_page_offset}\")\n",
    "   \n",
    "except Exception as e:\n",
    "   print(f\"Error writing output file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data has 1283 items\n",
      "Found 'open game license' in item - stopping processing here\n",
      "Cleaned data has 1113 items\n",
      "Processed until Open Game License: Yes\n",
      "Cleaned data saved to data/Legends/Lengends_combined_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "input_file = 'data/Legends/Lengends_combined.json'\n",
    "output_file = 'data/Legends/Lengends_combined_cleaned.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Original data has {len(data)} items\")\n",
    "\n",
    "cleaned_data = []\n",
    "ogl_found = False\n",
    "\n",
    "for item in data:\n",
    "    # Check if this item contains \"open game license\" - if so, stop processing\n",
    "    if 'text' in item and isinstance(item['text'], str):\n",
    "        if 'open game license' in item['text'].lower():\n",
    "            print(f\"Found 'open game license' in item - stopping processing here\")\n",
    "            ogl_found = True\n",
    "            break\n",
    "    \n",
    "    # Skip marginalia and figure chunk types\n",
    "    if item.get('chunk_type') in ['marginalia', 'figure']:\n",
    "        continue\n",
    "    \n",
    "    # Clean the text field\n",
    "    if 'text' in item and isinstance(item['text'], str):\n",
    "        text = item['text']\n",
    "        \n",
    "        # Remove \"Jarvin B\" followed by anything (using regex to handle variations)\n",
    "        text = re.sub(r'Jarvin B.*?(?=\\n|$)', '', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove \"Paizo Inc\"\n",
    "        text = re.sub(r'Paizo Inc[.,;]*', '', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove \"Wizards of the Coast\"\n",
    "        text = re.sub(r'Wizards of the Coast[.,;]*', '', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Clean up extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Update the item's text\n",
    "        item['text'] = text\n",
    "    \n",
    "    # Only keep items that still have meaningful text after cleaning\n",
    "    if item.get('text', '').strip():\n",
    "        cleaned_data.append(item)\n",
    "\n",
    "print(f\"Cleaned data has {len(cleaned_data)} items\")\n",
    "print(f\"Processed until Open Game License: {'Yes' if ogl_found else 'No (not found)'}\")\n",
    "\n",
    "# Save the cleaned data\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(cleaned_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract table of contents...by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
