{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list all unicode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique unicode characters found:\n",
      "U+00E7 : 'ç'\n",
      "U+00FC : 'ü'\n",
      "U+014C : 'Ō'\n",
      "U+2003 : '\\u2003'\n",
      "U+2013 : '–'\n",
      "U+2014 : '—'\n",
      "U+2019 : '’'\n",
      "U+201C : '“'\n",
      "U+201D : '”'\n",
      "U+2022 : '•'\n",
      "U+25C6 : '◆'\n",
      "U+25C7 : '◇'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Path to your JSON file\n",
    "json_path = 'data/Legends/Pathfinder Legends_1_50.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to extract all non-ASCII (unicode) characters from a string\n",
    "def extract_unicode(text):\n",
    "    return re.findall(r'[^\\x00-\\x7F]', text)\n",
    "\n",
    "# Collect all unicode characters from all text fields\n",
    "all_unicode_chars = set()\n",
    "for entry in data:\n",
    "    text = entry.get('text', '')\n",
    "    unicode_chars = extract_unicode(text)\n",
    "    all_unicode_chars.update(unicode_chars)\n",
    "\n",
    "# Print the unique unicode characters found\n",
    "print(\"Unique unicode characters found:\")\n",
    "for char in sorted(all_unicode_chars):\n",
    "    print(f\"U+{ord(char):04X} : {repr(char)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all unique chunk types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique chunk types found:\n",
      "figure\n",
      "marginalia\n",
      "table\n",
      "text\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSON file\n",
    "json_path = 'data/Legends/Pathfinder Legends_1_50.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Collect all unique chunk types\n",
    "chunk_types = set()\n",
    "for entry in data:\n",
    "    chunk_type = entry.get('chunk_type')\n",
    "    if chunk_type is not None:\n",
    "        chunk_types.add(chunk_type)\n",
    "\n",
    "# Print the unique chunk types\n",
    "print(\"Unique chunk types found:\")\n",
    "for ct in sorted(chunk_types):\n",
    "    print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings\n",
    "\n",
    "- can remove all of marginalia\n",
    "- need to parse and remove any mention of Jarvin\n",
    "- need to convert unicode\n",
    "- need to parse figure chunk type and only keep scene overview\n",
    "- need to stitch back into text or markdown for processing\n",
    "\n",
    "- need to decide where to split document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined json and fix page numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 JSON files in data/Legends\n",
      "Processing Pathfinder Legends_1_50.json...\n",
      "  Added 451 items, pages now offset by 52\n",
      "Processing Pathfinder Legends_51_100.json...\n",
      "  Added 527 items, pages now offset by 102\n",
      "Processing Pathfinder Legends_101_130.json...\n",
      "  Added 305 items, pages now offset by 132\n",
      "\n",
      "Successfully combined 3 files into data/Legends/Lengends_combined.json\n",
      "Total items: 1283\n",
      "Total pages: 132\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "directory_path = 'data/Legends'\n",
    "output_file = 'Lengends_combined.json'\n",
    "\n",
    "directory = Path(directory_path)\n",
    "\n",
    "if not directory.exists():\n",
    "   print(f\"Error: Directory '{directory_path}' does not exist\")\n",
    "\n",
    "# Find all JSON files matching the pattern\n",
    "json_files = list(directory.glob(\"Pathfinder Legends_*.json\"))\n",
    "\n",
    "if not json_files:\n",
    "   print(f\"No JSON files found matching pattern 'Pathfinder Legends_*.json' in {directory_path}\")\n",
    "\n",
    "# Sort files by the starting page number\n",
    "def extract_start_page(file_path):\n",
    "   match = re.search(r'_(\\d+)_\\d+\\.json$', file_path.name)\n",
    "   return int(match.group(1)) if match else 0\n",
    "\n",
    "json_files.sort(key=extract_start_page)\n",
    "\n",
    "combined_data = []\n",
    "current_page_offset = 0  # Start at 1 instead of 0\n",
    "\n",
    "print(f\"Found {len(json_files)} JSON files in {directory_path}\")\n",
    "\n",
    "for file_path in json_files:\n",
    "   print(f\"Processing {file_path.name}...\")\n",
    "   \n",
    "   try:\n",
    "       with open(file_path, 'r', encoding='utf-8') as f:\n",
    "           data = json.load(f)\n",
    "   except Exception as e:\n",
    "       print(f\"Error reading {file_path.name}: {e}\")\n",
    "       continue\n",
    "   \n",
    "   # Extract page range from filename\n",
    "   match = re.search(r'_(\\d+)_(\\d+)\\.json$', file_path.name)\n",
    "   if match:\n",
    "       start_page = int(match.group(1))\n",
    "       end_page = int(match.group(2))\n",
    "       original_page_count = end_page - start_page + 1\n",
    "   else:\n",
    "       # Fallback: count unique pages in the data\n",
    "       pages_in_file = set()\n",
    "       for item in data:\n",
    "           if 'grounding' in item:\n",
    "               for ground in item['grounding']:\n",
    "                   if 'page' in ground:\n",
    "                       pages_in_file.add(ground['page'])\n",
    "       original_page_count = len(pages_in_file) if pages_in_file else 1\n",
    "   \n",
    "   # Update page numbers and clean unicode in grounding data\n",
    "   for item in data:\n",
    "       # Clean unicode in text field\n",
    "       if 'text' in item and isinstance(item['text'], str):\n",
    "           # Normalize unicode and replace problematic characters\n",
    "           item['text'] = unicodedata.normalize('NFKD', item['text'])\n",
    "           item['text'] = item['text'].replace('\\u2022', '•').replace('\\u2013', '–').replace('\\u2014', '—')\n",
    "           item['text'] = item['text'].replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "           item['text'] = item['text'].replace('\\u201c', '\"').replace('\\u201d', '\"')\n",
    "           item['text'] = item['text'].replace('\\u2026', '...').replace('\\u00a0', ' ')\n",
    "       \n",
    "       if 'grounding' in item:\n",
    "           for grounding in item['grounding']:\n",
    "               if 'page' in grounding:\n",
    "                   grounding['page'] = grounding['page'] + current_page_offset\n",
    "   \n",
    "   # Add to combined data\n",
    "   combined_data.extend(data)\n",
    "   \n",
    "   # Update offset for next file\n",
    "   current_page_offset += original_page_count\n",
    "   print(f\"  Added {len(data)} items, pages now offset by {current_page_offset}\")\n",
    "\n",
    "# Write combined file to the same directory\n",
    "output_path = directory / output_file\n",
    "try:\n",
    "   with open(output_path, 'w', encoding='utf-8') as f:\n",
    "       json.dump(combined_data, f, indent=2, ensure_ascii=False)\n",
    "   \n",
    "   print(f\"\\nSuccessfully combined {len(json_files)} files into {output_path}\")\n",
    "   print(f\"Total items: {len(combined_data)}\")\n",
    "   print(f\"Total pages: {current_page_offset}\")\n",
    "   \n",
    "except Exception as e:\n",
    "   print(f\"Error writing output file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data has 1283 items\n",
      "Found 'open game license' in item - stopping processing here\n",
      "Cleaned data has 1113 items\n",
      "Processed until Open Game License: Yes\n",
      "Cleaned data saved to data/Legends/Lengends_combined_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "input_file = 'data/Legends/Lengends_combined.json'\n",
    "output_file = 'data/Legends/Lengends_combined_cleaned.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Original data has {len(data)} items\")\n",
    "\n",
    "cleaned_data = []\n",
    "ogl_found = False\n",
    "\n",
    "for item in data:\n",
    "    # Check if this item contains \"open game license\" - if so, stop processing\n",
    "    if 'text' in item and isinstance(item['text'], str):\n",
    "        if 'open game license' in item['text'].lower():\n",
    "            print(f\"Found 'open game license' in item - stopping processing here\")\n",
    "            ogl_found = True\n",
    "            break\n",
    "    \n",
    "    # Skip marginalia and figure chunk types\n",
    "    if item.get('chunk_type') in ['marginalia', 'figure']:\n",
    "        continue\n",
    "    \n",
    "    # Clean the text field\n",
    "    if 'text' in item and isinstance(item['text'], str):\n",
    "        text = item['text']\n",
    "        \n",
    "        # Remove \"Jarvin B\" followed by anything (using regex to handle variations)\n",
    "        text = re.sub(r'Jarvin B.*?(?=\\n|$)', '', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove \"Paizo Inc\"\n",
    "        text = re.sub(r'Paizo Inc[.,;]*', '', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove \"Wizards of the Coast\"\n",
    "        text = re.sub(r'Wizards of the Coast[.,;]*', '', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Clean up extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Update the item's text\n",
    "        item['text'] = text\n",
    "    \n",
    "    # Only keep items that still have meaningful text after cleaning\n",
    "    if item.get('text', '').strip():\n",
    "        cleaned_data.append(item)\n",
    "\n",
    "print(f\"Cleaned data has {len(cleaned_data)} items\")\n",
    "print(f\"Processed until Open Game License: {'Yes' if ogl_found else 'No (not found)'}\")\n",
    "\n",
    "# Save the cleaned data\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(cleaned_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract table of contents...by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_dict = {\n",
    "    \"INTRODUCTION\": 6,\n",
    "    \"ABROGAIL THRUNE II\": 8,\n",
    "    \"ANDIRA MARUSEK\": 12,\n",
    "    \"ANONG ARUNAK\": 14,\n",
    "    \"ARDAX THE WHITE-HAIR\": 16,\n",
    "    \"ARTOKUS KIRRAN\": 18,\n",
    "    \"AVARNEUS\": 22,\n",
    "    \"AZAERSI\": 26,\n",
    "    \"BABA YAGA\": 30,\n",
    "    \"BELIMARIUS AND SORSHEN\": 34,\n",
    "    \"CAMILIA DRANNOCH\": 38,\n",
    "    \"CHORAL THE CONQUEROR\": 40,\n",
    "    \"EUTROPIA STAVIAN\": 42,\n",
    "    \"GEB\": 44,\n",
    "    \"HAO JIN\": 46,\n",
    "    \"HASHIM IBN SAYYID\": 50,\n",
    "    \"IRABETH TIRABADE\": 52,\n",
    "    \"IRAHAI\": 54,\n",
    "    \"JAKALYN\": 56,\n",
    "    \"JANATIMO\": 60,\n",
    "    \"OLD-MAGE JATEMBE\": 62,\n",
    "    \"KALABRYNNE AND CLARETHE IOMEDAR\": 66,\n",
    "    \"KASSI AZIRIL\": 70,\n",
    "    \"KEVOTH-KUL\": 74,\n",
    "    \"KHISMAR CROOKCHAR\": 78,\n",
    "    \"LICKTOAD GOBLINS\": 82,\n",
    "    \"MAGDELENA AND MARTUM FALLOWS\": 84,\n",
    "    \"NANKOU\": 88,\n",
    "    \"NEX\": 90,\n",
    "    \"RAZMIR\": 92,\n",
    "    \"SAPPHIRE BUTTERFLY\": 94,\n",
    "    \"SHIMALI MANUX\": 96,\n",
    "    \"SIHAR\": 98,\n",
    "    \"TAARGICK\": 102,\n",
    "    \"TAR-BAPHON\": 104,\n",
    "    \"TELANDIA EDASSERIL\": 106,\n",
    "    \"TESSA FAIRWIND\": 108,\n",
    "    \"Thira Ash-Eyes\": 110,\n",
    "    \"Toulon Vidoc\": 112,\n",
    "    \"Ulthun II\": 114,\n",
    "    \"White Estrid\": 116,\n",
    "    \"Wynsal Starborn\": 118,\n",
    "    \"Xerbystes II, Hebizid Vraj, and Deena al-Parishat\": 120,\n",
    "    \"Entwined Destinies\": 124,\n",
    "    \"Glossary & Index\": 126\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate full json document by chapters then convert to markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = Path(\"data/distil/Legends\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(\"data/Legends/Lengends_combined_cleaned.json\", \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Function to get text entries for a specific page range\n",
    "def get_text_for_pages(start_page, end_page):\n",
    "    text_entries = []\n",
    "    for entry in json_data:\n",
    "        if \"grounding\" in entry and entry[\"grounding\"]:\n",
    "            page = entry[\"grounding\"][0].get(\"page\")\n",
    "            if page and start_page <= page < end_page:\n",
    "                text_entries.append(entry[\"text\"])\n",
    "    return \"\\n\\n\".join(text_entries)\n",
    "\n",
    "# Create markdown files for each chapter\n",
    "for chapter, page in contents_dict.items():\n",
    "    # Get the next page number to determine the range\n",
    "    next_page = None\n",
    "    for next_chapter, next_page_num in contents_dict.items():\n",
    "        if next_page_num > page:\n",
    "            if next_page is None or next_page_num < next_page:\n",
    "                next_page = next_page_num\n",
    "    \n",
    "    # If this is the last chapter, use a large number as the end page\n",
    "    if next_page is None:\n",
    "        next_page = 1000\n",
    "    \n",
    "    # Get the text content for this chapter\n",
    "    content = get_text_for_pages(page, next_page)\n",
    "    \n",
    "    # Create a safe filename from the chapter name\n",
    "    safe_filename = chapter.lower().replace(\" \", \"_\").replace(\"&\", \"and\")\n",
    "    filename = f\"{safe_filename}.md\"\n",
    "    \n",
    "    # Write the markdown file\n",
    "    with open(output_dir / filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"# {chapter}\\n\\n\")\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
