{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list all unicode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique unicode characters found:\n",
      "U+00E7 : 'ç'\n",
      "U+00FC : 'ü'\n",
      "U+014C : 'Ō'\n",
      "U+2003 : '\\u2003'\n",
      "U+2013 : '–'\n",
      "U+2014 : '—'\n",
      "U+2019 : '’'\n",
      "U+201C : '“'\n",
      "U+201D : '”'\n",
      "U+2022 : '•'\n",
      "U+25C6 : '◆'\n",
      "U+25C7 : '◇'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Path to your JSON file\n",
    "json_path = 'data/Legends/Pathfinder Legends_1_50.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to extract all non-ASCII (unicode) characters from a string\n",
    "def extract_unicode(text):\n",
    "    return re.findall(r'[^\\x00-\\x7F]', text)\n",
    "\n",
    "# Collect all unicode characters from all text fields\n",
    "all_unicode_chars = set()\n",
    "for entry in data:\n",
    "    text = entry.get('text', '')\n",
    "    unicode_chars = extract_unicode(text)\n",
    "    all_unicode_chars.update(unicode_chars)\n",
    "\n",
    "# Print the unique unicode characters found\n",
    "print(\"Unique unicode characters found:\")\n",
    "for char in sorted(all_unicode_chars):\n",
    "    print(f\"U+{ord(char):04X} : {repr(char)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all unique chunk types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique chunk types found:\n",
      "figure\n",
      "marginalia\n",
      "table\n",
      "text\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSON file\n",
    "json_path = 'data/Legends/Pathfinder Legends_1_50.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Collect all unique chunk types\n",
    "chunk_types = set()\n",
    "for entry in data:\n",
    "    chunk_type = entry.get('chunk_type')\n",
    "    if chunk_type is not None:\n",
    "        chunk_types.add(chunk_type)\n",
    "\n",
    "# Print the unique chunk types\n",
    "print(\"Unique chunk types found:\")\n",
    "for ct in sorted(chunk_types):\n",
    "    print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings\n",
    "\n",
    "- can remove all of marginalia\n",
    "- need to parse and remove any mention of Jarvin\n",
    "- need to convert unicode\n",
    "- need to parse figure chunk type and only keep scene overview\n",
    "- need to stitch back into text or markdown for processing\n",
    "\n",
    "- need to decide where to split document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined json and fix page numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 JSON files in data/GM Core\n",
      "Processing Pathfinder GM Core Condensed_1_50.json...\n",
      "  Added 626 items, pages now offset by 50\n",
      "Processing Pathfinder GM Core Condensed_51_100.json...\n",
      "  Added 710 items, pages now offset by 100\n",
      "Processing Pathfinder GM Core Condensed_101_150.json...\n",
      "  Added 799 items, pages now offset by 150\n",
      "Processing Pathfinder GM Core Condensed_151_200.json...\n",
      "  Added 750 items, pages now offset by 200\n",
      "Processing Pathfinder GM Core Condensed_201_250.json...\n",
      "  Added 768 items, pages now offset by 250\n",
      "Processing Pathfinder GM Core Condensed_251_300.json...\n",
      "  Added 858 items, pages now offset by 300\n",
      "Processing Pathfinder GM Core Condensed_301_333.json...\n",
      "  Added 431 items, pages now offset by 333\n",
      "\n",
      "Successfully combined 7 files into data/GM Core/GM_Core_combined.json\n",
      "Total items: 4942\n",
      "Total pages: 333\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "directory_path = 'data/GM Core'\n",
    "output_file = 'GM_Core_combined.json'\n",
    "\n",
    "directory = Path(directory_path)\n",
    "\n",
    "if not directory.exists():\n",
    "   print(f\"Error: Directory '{directory_path}' does not exist\")\n",
    "\n",
    "# Find all JSON files matching the pattern\n",
    "json_files = list(directory.glob(\"Pathfinder GM Core Condensed*.json\"))\n",
    "\n",
    "if not json_files:\n",
    "   print(f\"No JSON files found matching pattern 'Pathfinder GM Core Condensed*.json' in {directory_path}\")\n",
    "\n",
    "# Sort files by the starting page number\n",
    "def extract_start_page(file_path):\n",
    "   match = re.search(r'_(\\d+)_\\d+\\.json$', file_path.name)\n",
    "   return int(match.group(1)) if match else 0\n",
    "\n",
    "json_files.sort(key=extract_start_page)\n",
    "\n",
    "combined_data = []\n",
    "current_page_offset = 0  # Start at 1 instead of 0\n",
    "\n",
    "print(f\"Found {len(json_files)} JSON files in {directory_path}\")\n",
    "\n",
    "for file_path in json_files:\n",
    "   print(f\"Processing {file_path.name}...\")\n",
    "   \n",
    "   try:\n",
    "       with open(file_path, 'r', encoding='utf-8') as f:\n",
    "           data = json.load(f)\n",
    "   except Exception as e:\n",
    "       print(f\"Error reading {file_path.name}: {e}\")\n",
    "       continue\n",
    "   \n",
    "   # Extract page range from filename\n",
    "   match = re.search(r'_(\\d+)_(\\d+)\\.json$', file_path.name)\n",
    "   if match:\n",
    "       start_page = int(match.group(1))\n",
    "       end_page = int(match.group(2))\n",
    "       original_page_count = end_page - start_page + 1\n",
    "   else:\n",
    "       # Fallback: count unique pages in the data\n",
    "       pages_in_file = set()\n",
    "       for item in data:\n",
    "           if 'grounding' in item:\n",
    "               for ground in item['grounding']:\n",
    "                   if 'page' in ground:\n",
    "                       pages_in_file.add(ground['page'])\n",
    "       original_page_count = len(pages_in_file) if pages_in_file else 1\n",
    "   \n",
    "   # Update page numbers and clean unicode in grounding data\n",
    "   for item in data:\n",
    "       # Clean unicode in text field\n",
    "       if 'text' in item and isinstance(item['text'], str):\n",
    "           # Normalize unicode and replace problematic characters\n",
    "           item['text'] = unicodedata.normalize('NFKD', item['text'])\n",
    "           item['text'] = item['text'].replace('\\u2022', '•').replace('\\u2013', '–').replace('\\u2014', '—')\n",
    "           item['text'] = item['text'].replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "           item['text'] = item['text'].replace('\\u201c', '\"').replace('\\u201d', '\"')\n",
    "           item['text'] = item['text'].replace('\\u2026', '...').replace('\\u00a0', ' ')\n",
    "       \n",
    "       if 'grounding' in item:\n",
    "           for grounding in item['grounding']:\n",
    "               if 'page' in grounding:\n",
    "                   grounding['page'] = grounding['page'] + current_page_offset\n",
    "   \n",
    "   # Add to combined data\n",
    "   combined_data.extend(data)\n",
    "   \n",
    "   # Update offset for next file\n",
    "   current_page_offset += original_page_count\n",
    "   print(f\"  Added {len(data)} items, pages now offset by {current_page_offset}\")\n",
    "\n",
    "# Write combined file to the same directory\n",
    "output_path = directory / output_file\n",
    "try:\n",
    "   with open(output_path, 'w', encoding='utf-8') as f:\n",
    "       json.dump(combined_data, f, indent=2, ensure_ascii=False)\n",
    "   \n",
    "   print(f\"\\nSuccessfully combined {len(json_files)} files into {output_path}\")\n",
    "   print(f\"Total items: {len(combined_data)}\")\n",
    "   print(f\"Total pages: {current_page_offset}\")\n",
    "   \n",
    "except Exception as e:\n",
    "   print(f\"Error writing output file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix single pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing GM_Core_combined.json...\n",
      "\n",
      "Successfully processed GM_Core_combined.json into /Users/animaznman/Library/CloudStorage/OneDrive-Personal/Documents/Coding/USF Masters/Generative AI/PF_GM_aid/data/GM Core/GM_Core_processed.json\n",
      "Total items: 4942\n",
      "Page range: 0 - 332\n",
      "Total unique pages: 333\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration - modify these as needed\n",
    "input_file = '/Users/animaznman/Library/CloudStorage/OneDrive-Personal/Documents/Coding/USF Masters/Generative AI/PF_GM_aid/data/GM Core/GM_Core_combined.json'  # Change to your specific file\n",
    "output_file = 'GM_Core_processed.json'\n",
    "\n",
    "input_path = Path(input_file)\n",
    "\n",
    "if not input_path.exists():\n",
    "    print(f\"Error: File '{input_file}' does not exist\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Processing {input_path.name}...\")\n",
    "\n",
    "try:\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading {input_path.name}: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Clean unicode in the data without changing page numbers\n",
    "for item in data:\n",
    "    # Clean unicode in text field\n",
    "    if 'text' in item and isinstance(item['text'], str):\n",
    "        # Normalize unicode and replace problematic characters\n",
    "        item['text'] = unicodedata.normalize('NFKD', item['text'])\n",
    "        item['text'] = item['text'].replace('\\u2022', '•').replace('\\u2013', '–').replace('\\u2014', '—')\n",
    "        item['text'] = item['text'].replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "        item['text'] = item['text'].replace('\\u201c', '\"').replace('\\u201d', '\"')\n",
    "        item['text'] = item['text'].replace('\\u2026', '...').replace('\\u00a0', ' ')\n",
    "    \n",
    "    # Note: Page numbers in grounding are kept as-is (no offset applied)\n",
    "\n",
    "# Write processed file to the same directory as input\n",
    "output_path = input_path.parent / output_file\n",
    "try:\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {input_path.name} into {output_path}\")\n",
    "    print(f\"Total items: {len(data)}\")\n",
    "    \n",
    "    # Count unique pages for reference\n",
    "    pages_in_file = set()\n",
    "    for item in data:\n",
    "        if 'grounding' in item:\n",
    "            for ground in item['grounding']:\n",
    "                if 'page' in ground:\n",
    "                    pages_in_file.add(ground['page'])\n",
    "    \n",
    "    if pages_in_file:\n",
    "        print(f\"Page range: {min(pages_in_file)} - {max(pages_in_file)}\")\n",
    "        print(f\"Total unique pages: {len(pages_in_file)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error writing output file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data has 878 items\n",
      "Removing item containing: jarvin b\n",
      "Removing item containing: paizo\n",
      "Removing item containing: jarvin b\n",
      "Found 'open game license' in item - stopping processing here\n",
      "Cleaned data has 644 items\n",
      "Removed 234 items\n",
      "Processed until Open Game License: Yes\n",
      "Cleaned data saved to data/Plague/Plague_processed_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "input_file = 'data/GM Core/GM_Core_combined.json'\n",
    "output_file = 'data/GM Core/GM_Core_processed.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Original data has {len(data)} items\")\n",
    "\n",
    "cleaned_data = []\n",
    "ogl_found = False\n",
    "\n",
    "# Define all the text patterns to check for removal\n",
    "removal_patterns = [\n",
    "    r'jarvin b',\n",
    "    r'paizo',\n",
    "    r'wizards of the coast'\n",
    "]\n",
    "\n",
    "for item in data:\n",
    "    # Check if this item contains \"open game license\" - if so, stop processing\n",
    "    if 'text' in item and isinstance(item['text'], str):\n",
    "        if 'open game license' in item['text'].lower():\n",
    "            print(f\"Found 'open game license' in item - stopping processing here\")\n",
    "            ogl_found = True\n",
    "            break\n",
    "    \n",
    "    # Skip marginalia and figure chunk types\n",
    "    if item.get('chunk_type') in ['marginalia', 'figure']:\n",
    "        continue\n",
    "    \n",
    "    # Check if the text contains any of the removal patterns\n",
    "    should_remove = False\n",
    "    if 'text' in item and isinstance(item['text'], str):\n",
    "        text_lower = item['text'].lower()\n",
    "        for pattern in removal_patterns:\n",
    "            if re.search(pattern, text_lower):\n",
    "                print(f\"Removing item containing: {pattern}\")\n",
    "                should_remove = True\n",
    "                break\n",
    "    \n",
    "    # Only keep items that don't match removal patterns and have meaningful text\n",
    "    if not should_remove and item.get('text', '').strip():\n",
    "        cleaned_data.append(item)\n",
    "\n",
    "print(f\"Cleaned data has {len(cleaned_data)} items\")\n",
    "print(f\"Removed {len(data) - len(cleaned_data)} items\")\n",
    "print(f\"Processed until Open Game License: {'Yes' if ogl_found else 'No (not found)'}\")\n",
    "\n",
    "# Save the cleaned data\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(cleaned_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract table of contents...by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_dict = {\n",
    "    \"The Fall of Plaguestone\": 4,\n",
    "    \"Part 1: A Mysterious Murder\": 5,\n",
    "    \"Part 2: The Ranger's Request\": 25,\n",
    "    \"Part 3: Into Vilree's Lair\": 37,\n",
    "    \"Adventure Toolbox\": 51,\n",
    "    \"Etran's Folly Gazetteer\": 52,\n",
    "    \"NPCs Around Town\": 53,\n",
    "    \"Bort's Caravan and Crew\": 54,\n",
    "    \"Character Creation and Backgrounds\": 56,\n",
    "    \"Rules and Rewards\": 57,\n",
    "    \"Background Side Quests\": 60,\n",
    "    \"Deadly Flora\": 63,\n",
    "    \"Ooze, Blood\": 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_dict = {\n",
    "    \"INTRODUCTION\": 6,\n",
    "    \"ABROGAIL THRUNE II\": 8,\n",
    "    \"ANDIRA MARUSEK\": 12,\n",
    "    \"ANONG ARUNAK\": 14,\n",
    "    \"ARDAX THE WHITE-HAIR\": 16,\n",
    "    \"ARTOKUS KIRRAN\": 18,\n",
    "    \"AVARNEUS\": 22,\n",
    "    \"AZAERSI\": 26,\n",
    "    \"BABA YAGA\": 30,\n",
    "    \"BELIMARIUS AND SORSHEN\": 34,\n",
    "    \"CAMILIA DRANNOCH\": 38,\n",
    "    \"CHORAL THE CONQUEROR\": 40,\n",
    "    \"EUTROPIA STAVIAN\": 42,\n",
    "    \"GEB\": 44,\n",
    "    \"HAO JIN\": 46,\n",
    "    \"HASHIM IBN SAYYID\": 50,\n",
    "    \"IRABETH TIRABADE\": 52,\n",
    "    \"IRAHAI\": 54,\n",
    "    \"JAKALYN\": 56,\n",
    "    \"JANATIMO\": 60,\n",
    "    \"OLD-MAGE JATEMBE\": 62,\n",
    "    \"KALABRYNNE AND CLARETHE IOMEDAR\": 66,\n",
    "    \"KASSI AZIRIL\": 70,\n",
    "    \"KEVOTH-KUL\": 74,\n",
    "    \"KHISMAR CROOKCHAR\": 78,\n",
    "    \"LICKTOAD GOBLINS\": 82,\n",
    "    \"MAGDELENA AND MARTUM FALLOWS\": 84,\n",
    "    \"NANKOU\": 88,\n",
    "    \"NEX\": 90,\n",
    "    \"RAZMIR\": 92,\n",
    "    \"SAPPHIRE BUTTERFLY\": 94,\n",
    "    \"SHIMALI MANUX\": 96,\n",
    "    \"SIHAR\": 98,\n",
    "    \"TAARGICK\": 102,\n",
    "    \"TAR-BAPHON\": 104,\n",
    "    \"TELANDIA EDASSERIL\": 106,\n",
    "    \"TESSA FAIRWIND\": 108,\n",
    "    \"Thira Ash-Eyes\": 110,\n",
    "    \"Toulon Vidoc\": 112,\n",
    "    \"Ulthun II\": 114,\n",
    "    \"White Estrid\": 116,\n",
    "    \"Wynsal Starborn\": 118,\n",
    "    \"Xerbystes II, Hebizid Vraj, and Deena al-Parishat\": 120,\n",
    "    \"Entwined Destinies\": 124,\n",
    "    \"Glossary & Index\": 126\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate full json document by chapters then convert to markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = Path(\"data/distil/Plague\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(\"data/Plague/Plague_processed_cleaned.json\", \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Function to get text entries for a specific page range\n",
    "def get_text_for_pages(start_page, end_page):\n",
    "    text_entries = []\n",
    "    for entry in json_data:\n",
    "        if \"grounding\" in entry and entry[\"grounding\"]:\n",
    "            page = entry[\"grounding\"][0].get(\"page\")\n",
    "            if page and start_page <= page < end_page:\n",
    "                text_entries.append(entry[\"text\"])\n",
    "    return \"\\n\\n\".join(text_entries)\n",
    "\n",
    "# Create markdown files for each chapter\n",
    "for chapter, page in contents_dict.items():\n",
    "    # Get the next page number to determine the range\n",
    "    next_page = None\n",
    "    for next_chapter, next_page_num in contents_dict.items():\n",
    "        if next_page_num > page:\n",
    "            if next_page is None or next_page_num < next_page:\n",
    "                next_page = next_page_num\n",
    "    \n",
    "    # If this is the last chapter, use a large number as the end page\n",
    "    if next_page is None:\n",
    "        next_page = 1000\n",
    "    \n",
    "    # Get the text content for this chapter\n",
    "    content = get_text_for_pages(page, next_page)\n",
    "    \n",
    "    # Create a safe filename from the chapter name\n",
    "    safe_filename = chapter.lower().replace(\" \", \"_\").replace(\"&\", \"and\")\n",
    "    filename = f\"{safe_filename}.md\"\n",
    "    \n",
    "    # Write the markdown file\n",
    "    with open(output_dir / filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"# {chapter}\\n\\n\")\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split large md files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting part_1:_a_mysterious_murder.md (945 lines) into 10 parts...\n",
      "Created part_1:_a_mysterious_murder_part_1.md with 100 lines\n",
      "Created part_1:_a_mysterious_murder_part_2.md with 100 lines\n",
      "Created part_1:_a_mysterious_murder_part_3.md with 100 lines\n",
      "Created part_1:_a_mysterious_murder_part_4.md with 100 lines\n",
      "Created part_1:_a_mysterious_murder_part_5.md with 100 lines\n",
      "Created part_1:_a_mysterious_murder_part_6.md with 100 lines\n",
      "Created part_1:_a_mysterious_murder_part_7.md with 100 lines\n",
      "Created part_1:_a_mysterious_murder_part_8.md with 100 lines\n",
      "Created part_1:_a_mysterious_murder_part_9.md with 100 lines\n",
      "Created part_1:_a_mysterious_murder_part_10.md with 45 lines\n",
      "Splitting part_2:_the_ranger's_request.md (599 lines) into 6 parts...\n",
      "Created part_2:_the_ranger's_request_part_1.md with 100 lines\n",
      "Created part_2:_the_ranger's_request_part_2.md with 100 lines\n",
      "Created part_2:_the_ranger's_request_part_3.md with 100 lines\n",
      "Created part_2:_the_ranger's_request_part_4.md with 100 lines\n",
      "Created part_2:_the_ranger's_request_part_5.md with 100 lines\n",
      "Created part_2:_the_ranger's_request_part_6.md with 99 lines\n",
      "Splitting part_3:_into_vilree's_lair.md (749 lines) into 8 parts...\n",
      "Created part_3:_into_vilree's_lair_part_1.md with 100 lines\n",
      "Created part_3:_into_vilree's_lair_part_2.md with 100 lines\n",
      "Created part_3:_into_vilree's_lair_part_3.md with 100 lines\n",
      "Created part_3:_into_vilree's_lair_part_4.md with 100 lines\n",
      "Created part_3:_into_vilree's_lair_part_5.md with 100 lines\n",
      "Created part_3:_into_vilree's_lair_part_6.md with 100 lines\n",
      "Created part_3:_into_vilree's_lair_part_7.md with 100 lines\n",
      "Created part_3:_into_vilree's_lair_part_8.md with 49 lines\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def split_markdown_file(input_file, lines_per_chunk=100):\n",
    "    \"\"\"\n",
    "    Split a markdown file into multiple files with specified number of lines each.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input markdown file\n",
    "        lines_per_chunk (int): Number of lines per output file (default: 100)\n",
    "    \"\"\"\n",
    "    input_path = Path(input_file)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not input_path.exists():\n",
    "        print(f\"Error: File '{input_file}' does not exist\")\n",
    "        return\n",
    "    \n",
    "    # Read all lines from the input file\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return\n",
    "    \n",
    "    total_lines = len(lines)\n",
    "    total_parts = (total_lines + lines_per_chunk - 1) // lines_per_chunk  # Ceiling division\n",
    "    \n",
    "    print(f\"Splitting {input_path.name} ({total_lines} lines) into {total_parts} parts...\")\n",
    "    \n",
    "    # Get the base name without extension\n",
    "    base_name = input_path.stem\n",
    "    extension = input_path.suffix\n",
    "    output_dir = input_path.parent\n",
    "    \n",
    "    # Split the file\n",
    "    for part_num in range(1, total_parts + 1):\n",
    "        start_line = (part_num - 1) * lines_per_chunk\n",
    "        end_line = min(start_line + lines_per_chunk, total_lines)\n",
    "        \n",
    "        # Create output filename\n",
    "        output_filename = f\"{base_name}_part_{part_num}{extension}\"\n",
    "        output_path = output_dir / output_filename\n",
    "        \n",
    "        # Write the chunk to the new file\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.writelines(lines[start_line:end_line])\n",
    "            \n",
    "            lines_in_part = end_line - start_line\n",
    "            print(f\"Created {output_filename} with {lines_in_part} lines\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error writing {output_filename}: {e}\")\n",
    "    \n",
    "\n",
    "input_file = \"data/distil/Plague/part_1:_a_mysterious_murder.md\"\n",
    "split_markdown_file(input_file)\n",
    "\n",
    "input_file = \"data/distil/Plague/part_2:_the_ranger's_request.md\"\n",
    "split_markdown_file(input_file)\n",
    "\n",
    "input_file = \"data/distil/Plague/part_3:_into_vilree's_lair.md\"\n",
    "split_markdown_file(input_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
