{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list all unicode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique unicode characters found:\n",
      "U+00E7 : 'ç'\n",
      "U+00FC : 'ü'\n",
      "U+014C : 'Ō'\n",
      "U+2003 : '\\u2003'\n",
      "U+2013 : '–'\n",
      "U+2014 : '—'\n",
      "U+2019 : '’'\n",
      "U+201C : '“'\n",
      "U+201D : '”'\n",
      "U+2022 : '•'\n",
      "U+25C6 : '◆'\n",
      "U+25C7 : '◇'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Path to your JSON file\n",
    "json_path = 'data/Legends/Pathfinder Legends_1_50.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Function to extract all non-ASCII (unicode) characters from a string\n",
    "def extract_unicode(text):\n",
    "    return re.findall(r'[^\\x00-\\x7F]', text)\n",
    "\n",
    "# Collect all unicode characters from all text fields\n",
    "all_unicode_chars = set()\n",
    "for entry in data:\n",
    "    text = entry.get('text', '')\n",
    "    unicode_chars = extract_unicode(text)\n",
    "    all_unicode_chars.update(unicode_chars)\n",
    "\n",
    "# Print the unique unicode characters found\n",
    "print(\"Unique unicode characters found:\")\n",
    "for char in sorted(all_unicode_chars):\n",
    "    print(f\"U+{ord(char):04X} : {repr(char)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all unique chunk types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique chunk types found:\n",
      "figure\n",
      "marginalia\n",
      "table\n",
      "text\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSON file\n",
    "json_path = 'data/Legends/Pathfinder Legends_1_50.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Collect all unique chunk types\n",
    "chunk_types = set()\n",
    "for entry in data:\n",
    "    chunk_type = entry.get('chunk_type')\n",
    "    if chunk_type is not None:\n",
    "        chunk_types.add(chunk_type)\n",
    "\n",
    "# Print the unique chunk types\n",
    "print(\"Unique chunk types found:\")\n",
    "for ct in sorted(chunk_types):\n",
    "    print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings\n",
    "\n",
    "- can remove all of marginalia\n",
    "- need to parse and remove any mention of Jarvin\n",
    "- need to convert unicode\n",
    "- need to parse figure chunk type and only keep scene overview\n",
    "- need to stitch back into text or markdown for processing\n",
    "\n",
    "- need to decide where to split document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Output written to test.md\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Path to your JSON file\n",
    "json_path = 'data/Legends/Pathfinder Legends_1_50.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "processed_entries = []\n",
    "\n",
    "for entry in data:\n",
    "    # 1. Remove all marginalia\n",
    "    if entry.get('chunk_type') == 'marginalia':\n",
    "        continue\n",
    "\n",
    "    text = entry.get('text', '')\n",
    "\n",
    "    # 2. Remove any mention of Jarvin (case-insensitive)\n",
    "    text = re.sub(r'Jarvin', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # 3. Convert unicode to closest ASCII\n",
    "    text = unidecode(text)\n",
    "\n",
    "    # 4. For figure, only keep Scene Overview section\n",
    "    if entry.get('chunk_type') == 'figure':\n",
    "        # Extract \"Scene Overview\" section\n",
    "        match = re.search(r'Scene Overview\\s*:(.*?)(?:\\n[A-Z][a-zA-Z ]+ ?:|\\Z)', text, re.DOTALL)\n",
    "        if match:\n",
    "            text = match.group(1).strip()\n",
    "        else:\n",
    "            text = ''  # If no scene overview, skip text\n",
    "\n",
    "    # 5. Get the first page number for ordering\n",
    "    grounding = entry.get('grounding', [])\n",
    "    if grounding and isinstance(grounding, list) and 'page' in grounding[0]:\n",
    "        page = grounding[0]['page']\n",
    "    else:\n",
    "        page = float('inf')  # Put entries without page at the end\n",
    "\n",
    "    processed_entries.append((page, text))\n",
    "\n",
    "# 6. Sort by page number\n",
    "processed_entries.sort(key=lambda x: x[0])\n",
    "\n",
    "# 7. Write to markdown file\n",
    "with open('test.md', 'w', encoding='utf-8') as f:\n",
    "    for _, text in processed_entries:\n",
    "        if text.strip():\n",
    "            f.write(text.strip() + '\\n\\n')\n",
    "\n",
    "print(\"Done! Output written to test.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Output written to test.md\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from collections import defaultdict\n",
    "\n",
    "# Path to your JSON file\n",
    "json_path = 'data/Legends/Pathfinder Legends_1_50.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "processed_entries = []\n",
    "\n",
    "for entry in data:\n",
    "    # 1. Remove all marginalia\n",
    "    if entry.get('chunk_type') == 'marginalia':\n",
    "        continue\n",
    "\n",
    "    text = entry.get('text', '')\n",
    "\n",
    "    # 2. Remove any mention of Jarvin (case-insensitive)\n",
    "    text = re.sub(r'Jarvin', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # 3. Convert unicode to closest ASCII\n",
    "    text = unidecode(text)\n",
    "\n",
    "    # 4. For figure, only keep Scene Overview section and add a tag\n",
    "    is_figure = entry.get('chunk_type') == 'figure'\n",
    "    if is_figure:\n",
    "        match = re.search(r'Scene Overview\\s*:(.*?)(?:\\n[A-Z][a-zA-Z ]+ ?:|\\Z)', text, re.DOTALL)\n",
    "        if match:\n",
    "            text = match.group(1).strip()\n",
    "            # Add markdown-friendly tag for figure\n",
    "            text = '<!-- FIGURE -->\\n**[Figure]**\\n\\n' + text\n",
    "        else:\n",
    "            text = ''\n",
    "\n",
    "    # 5. Get the first page number for ordering\n",
    "    grounding = entry.get('grounding', [])\n",
    "    if grounding and isinstance(grounding, list) and 'page' in grounding[0]:\n",
    "        page = grounding[0]['page']\n",
    "    else:\n",
    "        page = float('inf')\n",
    "\n",
    "    processed_entries.append((page, text))\n",
    "\n",
    "# 6. Sort by page number\n",
    "processed_entries.sort(key=lambda x: x[0])\n",
    "\n",
    "# 7. Group entries by page\n",
    "page_texts = defaultdict(list)\n",
    "for page, text in processed_entries:\n",
    "    if text.strip() and page != float('inf'):\n",
    "        page_texts[page].append(text.strip())\n",
    "\n",
    "# 8. Write to markdown file with markdown-compatible page and figure tags\n",
    "with open('test.md', 'w', encoding='utf-8') as f:\n",
    "    for page in sorted(page_texts):\n",
    "        f.write(f'<!-- PAGE {page} -->\\n')\n",
    "        f.write(f'## Page {page}\\n')\n",
    "        f.write('---\\n\\n')\n",
    "        for text in page_texts[page]:\n",
    "            f.write(text + '\\n\\n')\n",
    "\n",
    "print(\"Done! Output written to test.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Combined JSON written to test.json and Markdown written to test.md\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from unidecode import unidecode\n",
    "from collections import defaultdict\n",
    "\n",
    "# Directory containing JSON files\n",
    "legends_dir = 'data/Legends'\n",
    "\n",
    "# List to hold all entries\n",
    "all_entries = []\n",
    "\n",
    "# Read all JSON files in the directory\n",
    "for filename in os.listdir(legends_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        # Parse the page range from the filename\n",
    "        match = re.search(r'Pathfinder Legends_(\\d+)_(\\d+)\\.json', filename)\n",
    "        if match:\n",
    "            start_page = int(match.group(1))\n",
    "            json_path = os.path.join(legends_dir, filename)\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for entry in data:\n",
    "                    # Adjust page numbers to start at 0\n",
    "                    grounding = entry.get('grounding', [])\n",
    "                    if grounding and isinstance(grounding, list) and 'page' in grounding[0]:\n",
    "                        entry['grounding'][0]['page'] -= start_page\n",
    "                    all_entries.append(entry)\n",
    "\n",
    "# Write combined JSON to test.json\n",
    "with open('test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_entries, f, indent=2)\n",
    "\n",
    "# Process combined JSON for Markdown output\n",
    "processed_entries = []\n",
    "\n",
    "for entry in all_entries:\n",
    "    # 1. Remove all marginalia\n",
    "    if entry.get('chunk_type') == 'marginalia':\n",
    "        continue\n",
    "\n",
    "    text = entry.get('text', '')\n",
    "\n",
    "    # 2. Remove any mention of Jarvin (case-insensitive)\n",
    "    text = re.sub(r'Jarvin', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # 3. Convert unicode to closest ASCII\n",
    "    text = unidecode(text)\n",
    "\n",
    "    # 4. For figure, only keep Scene Overview section and add a tag\n",
    "    is_figure = entry.get('chunk_type') == 'figure'\n",
    "    if is_figure:\n",
    "        match = re.search(r'Scene Overview\\s*:(.*?)(?:\\n[A-Z][a-zA-Z ]+ ?:|\\Z)', text, re.DOTALL)\n",
    "        if match:\n",
    "            text = match.group(1).strip()\n",
    "            text = '<!-- FIGURE -->\\n**[Figure]**\\n\\n' + text\n",
    "        else:\n",
    "            text = ''\n",
    "\n",
    "    # 5. Get the first page number for ordering\n",
    "    grounding = entry.get('grounding', [])\n",
    "    if grounding and isinstance(grounding, list) and 'page' in grounding[0]:\n",
    "        page = grounding[0]['page']\n",
    "    else:\n",
    "        page = float('inf')\n",
    "\n",
    "    processed_entries.append((page, text))\n",
    "\n",
    "# 6. Sort by page number\n",
    "processed_entries.sort(key=lambda x: x[0])\n",
    "\n",
    "# 7. Group entries by page\n",
    "page_texts = defaultdict(list)\n",
    "for page, text in processed_entries:\n",
    "    if text.strip() and page != float('inf'):\n",
    "        page_texts[page].append(text.strip())\n",
    "\n",
    "# 8. Write to markdown file with markdown-compatible page and figure tags\n",
    "with open('test.md', 'w', encoding='utf-8') as f:\n",
    "    for page in sorted(page_texts):\n",
    "        f.write(f'<!-- PAGE {page} -->\\n')\n",
    "        f.write(f'## Page {page}\\n')\n",
    "        f.write('---\\n\\n')\n",
    "        for text in page_texts[page]:\n",
    "            f.write(text + '\\n\\n')\n",
    "\n",
    "print(\"Done! Combined JSON written to test.json and Markdown written to test.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 JSON files in data/Legends\n",
      "Processing Pathfinder Legends_1_50.json...\n",
      "  Added 451 items, pages now offset by 51\n",
      "Processing Pathfinder Legends_51_100.json...\n",
      "  Added 527 items, pages now offset by 101\n",
      "Processing Pathfinder Legends_101_130.json...\n",
      "  Added 305 items, pages now offset by 131\n",
      "\n",
      "Successfully combined 3 files into data/Legends/test.json\n",
      "Total items: 1283\n",
      "Total pages: 131\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "directory_path = 'data/Legends'\n",
    "output_file = 'Lengends_combined.json'\n",
    "\n",
    "directory = Path(directory_path)\n",
    "\n",
    "if not directory.exists():\n",
    "   print(f\"Error: Directory '{directory_path}' does not exist\")\n",
    "\n",
    "# Find all JSON files matching the pattern\n",
    "json_files = list(directory.glob(\"Pathfinder Legends_*.json\"))\n",
    "\n",
    "if not json_files:\n",
    "   print(f\"No JSON files found matching pattern 'Pathfinder Legends_*.json' in {directory_path}\")\n",
    "\n",
    "# Sort files by the starting page number\n",
    "def extract_start_page(file_path):\n",
    "   match = re.search(r'_(\\d+)_\\d+\\.json$', file_path.name)\n",
    "   return int(match.group(1)) if match else 0\n",
    "\n",
    "json_files.sort(key=extract_start_page)\n",
    "\n",
    "combined_data = []\n",
    "current_page_offset = 1  # Start at 1 instead of 0\n",
    "\n",
    "print(f\"Found {len(json_files)} JSON files in {directory_path}\")\n",
    "\n",
    "for file_path in json_files:\n",
    "   print(f\"Processing {file_path.name}...\")\n",
    "   \n",
    "   try:\n",
    "       with open(file_path, 'r', encoding='utf-8') as f:\n",
    "           data = json.load(f)\n",
    "   except Exception as e:\n",
    "       print(f\"Error reading {file_path.name}: {e}\")\n",
    "       continue\n",
    "   \n",
    "   # Extract page range from filename\n",
    "   match = re.search(r'_(\\d+)_(\\d+)\\.json$', file_path.name)\n",
    "   if match:\n",
    "       start_page = int(match.group(1))\n",
    "       end_page = int(match.group(2))\n",
    "       original_page_count = end_page - start_page + 1\n",
    "   else:\n",
    "       # Fallback: count unique pages in the data\n",
    "       pages_in_file = set()\n",
    "       for item in data:\n",
    "           if 'grounding' in item:\n",
    "               for ground in item['grounding']:\n",
    "                   if 'page' in ground:\n",
    "                       pages_in_file.add(ground['page'])\n",
    "       original_page_count = len(pages_in_file) if pages_in_file else 1\n",
    "   \n",
    "   # Update page numbers and clean unicode in grounding data\n",
    "   for item in data:\n",
    "       # Clean unicode in text field\n",
    "       if 'text' in item and isinstance(item['text'], str):\n",
    "           # Normalize unicode and replace problematic characters\n",
    "           item['text'] = unicodedata.normalize('NFKD', item['text'])\n",
    "           item['text'] = item['text'].replace('\\u2022', '•').replace('\\u2013', '–').replace('\\u2014', '—')\n",
    "           item['text'] = item['text'].replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "           item['text'] = item['text'].replace('\\u201c', '\"').replace('\\u201d', '\"')\n",
    "           item['text'] = item['text'].replace('\\u2026', '...').replace('\\u00a0', ' ')\n",
    "       \n",
    "       if 'grounding' in item:\n",
    "           for grounding in item['grounding']:\n",
    "               if 'page' in grounding:\n",
    "                   grounding['page'] = grounding['page'] + current_page_offset\n",
    "   \n",
    "   # Add to combined data\n",
    "   combined_data.extend(data)\n",
    "   \n",
    "   # Update offset for next file\n",
    "   current_page_offset += original_page_count\n",
    "   print(f\"  Added {len(data)} items, pages now offset by {current_page_offset}\")\n",
    "\n",
    "# Write combined file to the same directory\n",
    "output_path = directory / output_file\n",
    "try:\n",
    "   with open(output_path, 'w', encoding='utf-8') as f:\n",
    "       json.dump(combined_data, f, indent=2, ensure_ascii=False)\n",
    "   \n",
    "   print(f\"\\nSuccessfully combined {len(json_files)} files into {output_path}\")\n",
    "   print(f\"Total items: {len(combined_data)}\")\n",
    "   print(f\"Total pages: {current_page_offset}\")\n",
    "   \n",
    "except Exception as e:\n",
    "   print(f\"Error writing output file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
