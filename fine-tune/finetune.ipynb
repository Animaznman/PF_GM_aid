{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e0d298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting unsloth\n",
      "  Downloading unsloth-2025.6.2-py3-none-any.whl (276 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.0/277.0 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wheel>=0.42.0\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/lib/python3/dist-packages (from unsloth) (5.9.0)\n",
      "Requirement already satisfied: torch<=2.7.0,>=2.4.0 in /usr/lib/python3/dist-packages (from unsloth) (2.6.0)\n",
      "Collecting hf_transfer\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.34.1\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting unsloth_zoo>=2025.6.1\n",
      "  Downloading unsloth_zoo-2025.6.1-py3-none-any.whl (147 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.4/147.4 KB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diffusers\n",
      "  Downloading diffusers-0.33.1-py3-none-any.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets>=3.4.1\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/lib/python3/dist-packages (from unsloth) (4.21.12)\n",
      "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9\n",
      "  Downloading trl-0.18.2-py3-none-any.whl (366 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.4/366.4 KB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.8/514.8 KB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/lib/python3/dist-packages (from unsloth) (1.21.5)\n",
      "Collecting sentencepiece>=0.2.0\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting xformers>=0.0.27.post2\n",
      "  Downloading xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl (31.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: triton>=3.0.0 in /usr/lib/python3/dist-packages (from unsloth) (3.2.0)\n",
      "Collecting peft!=0.11.0,>=0.7.1\n",
      "  Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 KB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/lib/python3/dist-packages (from unsloth) (21.3)\n",
      "Requirement already satisfied: torchvision in /usr/lib/python3/dist-packages (from unsloth) (0.21.0)\n",
      "Collecting tyro\n",
      "  Downloading tyro-0.9.24-py3-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.3/128.3 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 KB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate>=0.34.1->unsloth) (5.4.1)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (from datasets>=3.4.1->unsloth) (1.3.5)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.32.2\n",
      "  Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets>=3.4.1->unsloth) (3.6.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from datasets>=3.4.1->unsloth) (2024.3.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2\n",
      "  Downloading hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface_hub->unsloth) (4.10.0)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow in /usr/lib/python3/dist-packages (from unsloth_zoo>=2025.6.1->unsloth) (9.0.1)\n",
      "Collecting cut_cross_entropy\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting msgspec\n",
      "  Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 KB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch<=2.7.0,>=2.4.0\n",
      "  Downloading torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (865.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.4.2\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.6.77\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.5.1.17\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.6.85\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufile-cu12==1.11.1.6\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.3\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth) (2.4)\n",
      "Collecting nvidia-nccl-cu12==2.26.2\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.3.0.4\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.6.77\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.6.77\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 KB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.6.4.1\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth) (3.0.3)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.7.77\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton>=3.0.0\n",
      "  Downloading triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.7.1.2\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton>=3.0.0->unsloth) (59.6.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers->unsloth) (4.6.4)\n",
      "Collecting shtab>=1.5.6\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/lib/python3/dist-packages (from tyro->unsloth) (11.2.0)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typeguard>=4.0.0\n",
      "  Downloading typeguard-4.4.3-py3-none-any.whl (34 kB)\n",
      "Collecting docstring-parser>=0.15\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.12.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth) (2020.6.20)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth) (3.3)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/lib/python3/dist-packages (from rich>=11.1.0->tyro->unsloth) (0.9.1)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /usr/lib/python3/dist-packages (from rich>=11.1.0->tyro->unsloth) (0.4.4)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/lib/python3/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.11.2)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.3/198.3 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (21.2.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.9/222.9 KB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.1/326.1 KB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: sentencepiece, nvidia-cusparselt-cu12, mpmath, xxhash, wheel, typing-extensions, triton, tqdm, sympy, shtab, safetensors, regex, pyarrow, protobuf, propcache, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, hf-xet, hf_transfer, frozenlist, docstring-parser, dill, charset_normalizer, async-timeout, aiohappyeyeballs, typeguard, requests, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, multidict, aiosignal, yarl, tyro, nvidia-cusolver-cu12, huggingface_hub, torch, tokenizers, diffusers, aiohttp, xformers, transformers, cut_cross_entropy, bitsandbytes, accelerate, peft, datasets, trl, unsloth_zoo, unsloth\n",
      "Successfully installed accelerate-1.7.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.3.2 async-timeout-5.0.1 bitsandbytes-0.46.0 charset_normalizer-3.4.2 cut_cross_entropy-25.1.1 datasets-3.6.0 diffusers-0.33.1 dill-0.3.8 docstring-parser-0.16 frozenlist-1.7.0 hf-xet-1.1.4 hf_transfer-0.1.9 huggingface_hub-0.33.0 mpmath-1.3.0 msgspec-0.19.0 multidict-6.4.4 multiprocess-0.70.16 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 peft-0.15.2 propcache-0.3.2 protobuf-3.20.3 pyarrow-20.0.0 regex-2024.11.6 requests-2.32.4 safetensors-0.5.3 sentencepiece-0.2.0 shtab-1.7.2 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.0 tqdm-4.67.1 transformers-4.52.4 triton-3.3.0 trl-0.18.2 typeguard-4.4.3 typing-extensions-4.14.0 tyro-0.9.24 unsloth-2025.6.2 unsloth_zoo-2025.6.1 wheel-0.45.1 xformers-0.0.30 xxhash-3.5.0 yarl-1.20.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tf-keras==2.16.0\n",
      "  Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.16.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting Pillow>=10.0.0\n",
      "  Downloading pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Pillow\n",
      "Successfully installed Pillow-11.2.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting widgetsnbextension~=4.0.14\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ipython>=6.1.0 in /usr/lib/python3/dist-packages (from ipywidgets) (7.31.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/lib/python3/dist-packages (from ipywidgets) (5.1.1)\n",
      "Collecting jupyterlab_widgets~=3.0.15\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting comm>=0.1.3\n",
      "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, comm, ipywidgets\n",
      "Successfully installed comm-0.2.2 ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth\n",
    "!pip install tf-keras==2.16.0 --no-dependencies\n",
    "!pip install \"Pillow>=10.0.0\"\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4159621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.2: Fast Llama patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    Quadro RTX 6000. Num GPUs = 1. Max memory: 23.461 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6f4c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c93b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79 QA pair files\n",
      "\n",
      "Dataset statistics:\n",
      "Total number of examples: 717\n",
      "\n",
      "Sample entry:\n",
      "{'instruction': \"What does Rolth do as the town's member of law enforcement?\", 'output': 'Rolth Garley settles disputes over livestock and breaks up the occasional drunken brawl.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Function to convert QA pairs to Alpaca format\n",
    "def convert_to_alpaca_format(qa_pairs):\n",
    "    alpaca_data = []\n",
    "    \n",
    "    for pair in qa_pairs:\n",
    "        # Create Alpaca format entry\n",
    "        entry = {\n",
    "            \"instruction\": pair[\"question\"],\n",
    "            \"output\": pair[\"answer\"]\n",
    "        }\n",
    "        alpaca_data.append(entry)\n",
    "    \n",
    "    return alpaca_data\n",
    "\n",
    "# Load the JSON file\n",
    "def load_qa_pairs(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        qa_pairs = json.load(f)\n",
    "    return qa_pairs\n",
    "\n",
    "# Main processing\n",
    "def create_alpaca_dataset(directory_path):\n",
    "    all_alpaca_data = []\n",
    "    \n",
    "    # Find all qa_pairs.json files in the directory and subdirectories\n",
    "    json_files = glob(os.path.join(directory_path, \"**/*qa_pairs.json\"), recursive=True)\n",
    "    \n",
    "    print(f\"Found {len(json_files)} QA pair files\")\n",
    "    \n",
    "    # Process each file\n",
    "    for json_file in json_files:\n",
    "        qa_pairs = load_qa_pairs(json_file)\n",
    "        alpaca_data = convert_to_alpaca_format(qa_pairs)\n",
    "        all_alpaca_data.extend(alpaca_data)\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    df = pd.DataFrame(all_alpaca_data)\n",
    "    \n",
    "    # Convert to HuggingFace Dataset\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Example usage\n",
    "# directory_path = \"/home/ubuntu/PF_GM_aid/data/QA Pairs\"  # Adjust this to your QA pairs directory\n",
    "directory_path = '/Users/nicholasbarsi-rhyne/Documents/School/Graduate/Summer 25/GenAI/PF_GM_aid/data/QA Pairs'\n",
    "dataset = create_alpaca_dataset(directory_path)\n",
    "\n",
    "# Print some statistics\n",
    "print(\"\\nDataset statistics:\")\n",
    "print(f\"Total number of examples: {len(dataset)}\")\n",
    "print(\"\\nSample entry:\")\n",
    "print(dataset[0])\n",
    "\n",
    "# Save the dataset if needed\n",
    "# dataset.save_to_disk(\"path/to/save/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "361c2d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Who are Magdelena and Martum Fallows?',\n",
       " 'output': 'They are halfling twins who serve as the current \"Farmer,\" the leader of the Bellflower Network, an abolitionist organization working to free halfling slaves in Cheliax.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ad54bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \"\"\"<|im_start|>system\n",
    "{SYSTEM}<|im_end|>\n",
    "<|im_start|>user\n",
    "{INPUT}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{OUTPUT}<|im_end|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "045ebf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285622f1667d413ebf9cca1a57c046a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/717 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_chat_template(examples):\n",
    "    chat_template = \"\"\"<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "{INPUT}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{OUTPUT}<|im_end|>\"\"\"\n",
    "    \n",
    "    texts = []\n",
    "    for i in range(len(examples[\"instruction\"])):\n",
    "        text = chat_template.format(\n",
    "            INPUT=examples[\"instruction\"][i],\n",
    "            OUTPUT=examples[\"output\"][i]\n",
    "        )\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(format_chat_template, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21803a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset_split['train']\n",
    "val_dataset = dataset_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a0680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca66e683e2174c9c954ba777ec830e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/717 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    add_special_tokens=True,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 20,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 5e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a40237e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 717 | Num Epochs = 4 | Total steps = 360\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 24,313,856/3,000,000,000 (0.81% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [360/360 08:26, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.150200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.108600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.783100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.888800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.730600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.420900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.864300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.953200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.783800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.652400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.843800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.703200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.862200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.702100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.756500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.719300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.498200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.651300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.687800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.532300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.725600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.653500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.754000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.620200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.655900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.689400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.579500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.657100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.742200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.495500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.718400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.893700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.707800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.303400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.421100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.549700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.588500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.620700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.845100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.551300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.343800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.367500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.548500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.710800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.318800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.475100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.415800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.810300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.396300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.526100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.558400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.669600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.309500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.578400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.404300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.387700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.399700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.533600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.314700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.375600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.575500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.284500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.283500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.452300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.468400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.461900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.291200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.185900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.326200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.187200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.333700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.384800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.244300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.230600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.232600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.338200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.380200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.199700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.253900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.561200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.178700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.220500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.191300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.444200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.963900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.274500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>1.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>1.285500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.237100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.323200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.248300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.292500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.313200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>1.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.379700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>1.329700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.848300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>1.305500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.323800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.357100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.218100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>1.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>1.290100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>1.319100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.222500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>1.390600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.207300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>1.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.160200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>1.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>1.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>1.315500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>1.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>1.228600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>1.255200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>1.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1.167600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.132600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>1.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1.238600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>1.277800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>1.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>1.255500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>1.288500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>1.360800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.045600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1.296200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>1.386700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>1.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>1.183400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.220400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.889900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.976900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.932200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.976300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>1.125100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.879900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.965300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.932200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>1.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.963900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.921300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>1.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.874800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.850800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.940100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>1.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.766300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.926400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.947200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.956300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.941300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>1.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>1.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.900400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.899400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>1.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.790200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>1.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>1.196100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>1.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.967900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>1.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.869400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.970100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>1.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>1.042900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.969700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>1.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>1.154300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.956700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.995700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.974600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.974400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>1.034400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.875700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>1.164500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>1.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.857200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.925100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.956400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.917500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>1.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.860600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>1.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>1.142500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.953900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>1.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.989200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.888600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.995700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>1.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>1.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>1.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>1.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>1.037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.917600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.805800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.935300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>1.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.891300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.990600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>1.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>1.037200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.843600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.967400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.989100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.794900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>1.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.928100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.797300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.758200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.893900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.846800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.642300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.717700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.826300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.731300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.774400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.801900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>0.819600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.756700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.914500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.624000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>0.651600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>0.833800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.777300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.718700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.736700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.828000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>0.563800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>0.828100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>0.731400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>0.797800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.809200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.820200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>0.800500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>0.775300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.793200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>0.770200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.702700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>0.829300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.929100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>0.835300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>0.681700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.655400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>0.580100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>0.679200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>0.768700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>0.742300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.715200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>0.733400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>0.690500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>0.762500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>0.670400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.721400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>0.579900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>0.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.748600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>0.830600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.750200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>0.738200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>0.651100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>0.752500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>0.870400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.731400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.743200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>0.840400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>0.743600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>0.685700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.855300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>0.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>0.793300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>0.729900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>0.814600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>0.839800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>0.848500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>0.729200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>0.867200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.720800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>0.770500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.959600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>0.770300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>0.731300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.754700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>0.734400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>0.794600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>0.718800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>0.744200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.648500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a72884aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training logs: [{'loss': 3.2328, 'grad_norm': 1.8696959018707275, 'learning_rate': 0.0, 'epoch': 0.011142061281337047, 'step': 1}, {'loss': 3.4345, 'grad_norm': 1.965870976448059, 'learning_rate': 2e-05, 'epoch': 0.022284122562674095, 'step': 2}, {'loss': 3.1502, 'grad_norm': 1.6770148277282715, 'learning_rate': 4e-05, 'epoch': 0.033426183844011144, 'step': 3}, {'loss': 3.1544, 'grad_norm': 1.4508470296859741, 'learning_rate': 6e-05, 'epoch': 0.04456824512534819, 'step': 4}, {'loss': 3.1086, 'grad_norm': 1.147133708000183, 'learning_rate': 8e-05, 'epoch': 0.055710306406685235, 'step': 5}, {'loss': 2.7831, 'grad_norm': 1.0745567083358765, 'learning_rate': 0.0001, 'epoch': 0.06685236768802229, 'step': 6}, {'loss': 2.8888, 'grad_norm': 1.0245816707611084, 'learning_rate': 0.00012, 'epoch': 0.07799442896935933, 'step': 7}, {'loss': 2.7306, 'grad_norm': 1.0352387428283691, 'learning_rate': 0.00014, 'epoch': 0.08913649025069638, 'step': 8}, {'loss': 2.4209, 'grad_norm': 1.038892149925232, 'learning_rate': 0.00016, 'epoch': 0.10027855153203342, 'step': 9}, {'loss': 2.2131, 'grad_norm': 0.9406212568283081, 'learning_rate': 0.00018, 'epoch': 0.11142061281337047, 'step': 10}, {'loss': 2.2352, 'grad_norm': 1.0952033996582031, 'learning_rate': 0.0002, 'epoch': 0.12256267409470752, 'step': 11}, {'loss': 2.0463, 'grad_norm': 1.301088809967041, 'learning_rate': 0.00019942857142857143, 'epoch': 0.13370473537604458, 'step': 12}, {'loss': 2.0259, 'grad_norm': 0.9689923524856567, 'learning_rate': 0.00019885714285714287, 'epoch': 0.14484679665738162, 'step': 13}, {'loss': 1.8643, 'grad_norm': 0.725860595703125, 'learning_rate': 0.0001982857142857143, 'epoch': 0.15598885793871867, 'step': 14}, {'loss': 1.9532, 'grad_norm': 0.6502383947372437, 'learning_rate': 0.0001977142857142857, 'epoch': 0.1671309192200557, 'step': 15}, {'loss': 1.7838, 'grad_norm': 0.5792446136474609, 'learning_rate': 0.00019714285714285716, 'epoch': 0.17827298050139276, 'step': 16}, {'loss': 1.6524, 'grad_norm': 0.5484322309494019, 'learning_rate': 0.00019657142857142858, 'epoch': 0.1894150417827298, 'step': 17}, {'loss': 1.728, 'grad_norm': 0.6956892013549805, 'learning_rate': 0.000196, 'epoch': 0.20055710306406685, 'step': 18}, {'loss': 1.8438, 'grad_norm': 0.5913244485855103, 'learning_rate': 0.00019542857142857144, 'epoch': 0.2116991643454039, 'step': 19}, {'loss': 1.7032, 'grad_norm': 0.7673594355583191, 'learning_rate': 0.00019485714285714286, 'epoch': 0.22284122562674094, 'step': 20}, {'loss': 1.8622, 'grad_norm': 0.6272003650665283, 'learning_rate': 0.0001942857142857143, 'epoch': 0.233983286908078, 'step': 21}, {'loss': 1.7021, 'grad_norm': 0.6740326285362244, 'learning_rate': 0.00019371428571428572, 'epoch': 0.24512534818941503, 'step': 22}, {'loss': 1.7565, 'grad_norm': 0.6815906167030334, 'learning_rate': 0.00019314285714285717, 'epoch': 0.2562674094707521, 'step': 23}, {'loss': 1.7193, 'grad_norm': 0.8186187148094177, 'learning_rate': 0.00019257142857142859, 'epoch': 0.26740947075208915, 'step': 24}, {'loss': 1.63, 'grad_norm': 0.8173750042915344, 'learning_rate': 0.000192, 'epoch': 0.2785515320334262, 'step': 25}, {'loss': 1.4982, 'grad_norm': 0.8509571552276611, 'learning_rate': 0.00019142857142857145, 'epoch': 0.28969359331476324, 'step': 26}, {'loss': 1.7536, 'grad_norm': 0.9601326584815979, 'learning_rate': 0.00019085714285714287, 'epoch': 0.3008356545961003, 'step': 27}, {'loss': 1.6513, 'grad_norm': 1.0021201372146606, 'learning_rate': 0.0001902857142857143, 'epoch': 0.31197771587743733, 'step': 28}, {'loss': 1.6878, 'grad_norm': 0.9898874759674072, 'learning_rate': 0.00018971428571428573, 'epoch': 0.3231197771587744, 'step': 29}, {'loss': 1.5323, 'grad_norm': 0.6925845146179199, 'learning_rate': 0.00018914285714285715, 'epoch': 0.3342618384401114, 'step': 30}, {'loss': 1.7256, 'grad_norm': 0.642378568649292, 'learning_rate': 0.00018857142857142857, 'epoch': 0.34540389972144847, 'step': 31}, {'loss': 1.7092, 'grad_norm': 0.5685174465179443, 'learning_rate': 0.000188, 'epoch': 0.3565459610027855, 'step': 32}, {'loss': 1.6535, 'grad_norm': 0.5492005348205566, 'learning_rate': 0.00018742857142857143, 'epoch': 0.36768802228412256, 'step': 33}, {'loss': 1.754, 'grad_norm': 0.7320264577865601, 'learning_rate': 0.00018685714285714285, 'epoch': 0.3788300835654596, 'step': 34}, {'loss': 1.6202, 'grad_norm': 0.5843124985694885, 'learning_rate': 0.0001862857142857143, 'epoch': 0.38997214484679665, 'step': 35}, {'loss': 1.765, 'grad_norm': 0.5871212482452393, 'learning_rate': 0.00018571428571428572, 'epoch': 0.4011142061281337, 'step': 36}, {'loss': 1.6559, 'grad_norm': 0.5687923431396484, 'learning_rate': 0.00018514285714285716, 'epoch': 0.41225626740947074, 'step': 37}, {'loss': 1.6894, 'grad_norm': 0.5598924160003662, 'learning_rate': 0.00018457142857142858, 'epoch': 0.4233983286908078, 'step': 38}, {'loss': 1.5795, 'grad_norm': 0.5656481981277466, 'learning_rate': 0.00018400000000000003, 'epoch': 0.43454038997214484, 'step': 39}, {'loss': 1.6571, 'grad_norm': 0.5999623537063599, 'learning_rate': 0.00018342857142857145, 'epoch': 0.4456824512534819, 'step': 40}, {'loss': 1.7422, 'grad_norm': 0.6236245632171631, 'learning_rate': 0.00018285714285714286, 'epoch': 0.4568245125348189, 'step': 41}, {'loss': 1.4955, 'grad_norm': 0.705866277217865, 'learning_rate': 0.0001822857142857143, 'epoch': 0.467966573816156, 'step': 42}, {'loss': 1.7184, 'grad_norm': 0.6486619710922241, 'learning_rate': 0.00018171428571428573, 'epoch': 0.479108635097493, 'step': 43}, {'loss': 1.8937, 'grad_norm': 0.7355676293373108, 'learning_rate': 0.00018114285714285715, 'epoch': 0.49025069637883006, 'step': 44}, {'loss': 1.4105, 'grad_norm': 0.7011520862579346, 'learning_rate': 0.00018057142857142857, 'epoch': 0.5013927576601671, 'step': 45}, {'loss': 1.7078, 'grad_norm': 0.6817482113838196, 'learning_rate': 0.00018, 'epoch': 0.5125348189415042, 'step': 46}, {'loss': 1.3034, 'grad_norm': 0.706348180770874, 'learning_rate': 0.00017942857142857143, 'epoch': 0.5236768802228412, 'step': 47}, {'loss': 1.3821, 'grad_norm': 0.5811812281608582, 'learning_rate': 0.00017885714285714285, 'epoch': 0.5348189415041783, 'step': 48}, {'loss': 1.4211, 'grad_norm': 0.6385146379470825, 'learning_rate': 0.0001782857142857143, 'epoch': 0.5459610027855153, 'step': 49}, {'loss': 1.5497, 'grad_norm': 0.7136363983154297, 'learning_rate': 0.0001777142857142857, 'epoch': 0.5571030640668524, 'step': 50}, {'loss': 1.5885, 'grad_norm': 0.6548654437065125, 'learning_rate': 0.00017714285714285713, 'epoch': 0.5682451253481894, 'step': 51}, {'loss': 1.6207, 'grad_norm': 0.6526280641555786, 'learning_rate': 0.00017657142857142858, 'epoch': 0.5793871866295265, 'step': 52}, {'loss': 1.8451, 'grad_norm': 0.7991377711296082, 'learning_rate': 0.00017600000000000002, 'epoch': 0.5905292479108635, 'step': 53}, {'loss': 1.5513, 'grad_norm': 0.7576894164085388, 'learning_rate': 0.00017542857142857144, 'epoch': 0.6016713091922006, 'step': 54}, {'loss': 1.3438, 'grad_norm': 0.7245895266532898, 'learning_rate': 0.0001748571428571429, 'epoch': 0.6128133704735376, 'step': 55}, {'loss': 1.3675, 'grad_norm': 0.6112740635871887, 'learning_rate': 0.0001742857142857143, 'epoch': 0.6239554317548747, 'step': 56}, {'loss': 1.5485, 'grad_norm': 0.6720521450042725, 'learning_rate': 0.00017371428571428572, 'epoch': 0.6350974930362117, 'step': 57}, {'loss': 1.3807, 'grad_norm': 0.6190703511238098, 'learning_rate': 0.00017314285714285717, 'epoch': 0.6462395543175488, 'step': 58}, {'loss': 1.7108, 'grad_norm': 0.6557987928390503, 'learning_rate': 0.0001725714285714286, 'epoch': 0.6573816155988857, 'step': 59}, {'loss': 1.3188, 'grad_norm': 0.7320505380630493, 'learning_rate': 0.000172, 'epoch': 0.6685236768802229, 'step': 60}, {'loss': 1.4751, 'grad_norm': 0.6003980040550232, 'learning_rate': 0.00017142857142857143, 'epoch': 0.6796657381615598, 'step': 61}, {'loss': 1.4158, 'grad_norm': 0.5645894408226013, 'learning_rate': 0.00017085714285714287, 'epoch': 0.6908077994428969, 'step': 62}, {'loss': 1.4635, 'grad_norm': 0.7726033926010132, 'learning_rate': 0.0001702857142857143, 'epoch': 0.7019498607242339, 'step': 63}, {'loss': 1.8103, 'grad_norm': 0.7319900393486023, 'learning_rate': 0.0001697142857142857, 'epoch': 0.713091922005571, 'step': 64}, {'loss': 1.3963, 'grad_norm': 0.7228984236717224, 'learning_rate': 0.00016914285714285715, 'epoch': 0.724233983286908, 'step': 65}, {'loss': 1.5261, 'grad_norm': 0.6437210440635681, 'learning_rate': 0.00016857142857142857, 'epoch': 0.7353760445682451, 'step': 66}, {'loss': 1.5584, 'grad_norm': 0.6872563362121582, 'learning_rate': 0.000168, 'epoch': 0.7465181058495822, 'step': 67}, {'loss': 1.2348, 'grad_norm': 0.7587931752204895, 'learning_rate': 0.00016742857142857144, 'epoch': 0.7576601671309192, 'step': 68}, {'loss': 1.6696, 'grad_norm': 0.7887881994247437, 'learning_rate': 0.00016685714285714285, 'epoch': 0.7688022284122563, 'step': 69}, {'loss': 1.3095, 'grad_norm': 0.6636202931404114, 'learning_rate': 0.0001662857142857143, 'epoch': 0.7799442896935933, 'step': 70}, {'loss': 1.39, 'grad_norm': 0.6598525643348694, 'learning_rate': 0.00016571428571428575, 'epoch': 0.7910863509749304, 'step': 71}, {'loss': 1.5784, 'grad_norm': 0.7077511548995972, 'learning_rate': 0.00016514285714285716, 'epoch': 0.8022284122562674, 'step': 72}, {'loss': 1.4043, 'grad_norm': 0.7556875944137573, 'learning_rate': 0.00016457142857142858, 'epoch': 0.8133704735376045, 'step': 73}, {'loss': 1.42, 'grad_norm': 0.7013286352157593, 'learning_rate': 0.000164, 'epoch': 0.8245125348189415, 'step': 74}, {'loss': 1.3877, 'grad_norm': 0.6574162840843201, 'learning_rate': 0.00016342857142857145, 'epoch': 0.8356545961002786, 'step': 75}, {'loss': 1.3997, 'grad_norm': 0.6343709230422974, 'learning_rate': 0.00016285714285714287, 'epoch': 0.8467966573816156, 'step': 76}, {'loss': 1.5, 'grad_norm': 0.7332975268363953, 'learning_rate': 0.00016228571428571428, 'epoch': 0.8579387186629527, 'step': 77}, {'loss': 1.5336, 'grad_norm': 0.7917966246604919, 'learning_rate': 0.00016171428571428573, 'epoch': 0.8690807799442897, 'step': 78}, {'loss': 1.3147, 'grad_norm': 0.6235187649726868, 'learning_rate': 0.00016114285714285715, 'epoch': 0.8802228412256268, 'step': 79}, {'loss': 1.304, 'grad_norm': 0.6637819409370422, 'learning_rate': 0.00016057142857142857, 'epoch': 0.8913649025069638, 'step': 80}, {'loss': 1.3756, 'grad_norm': 0.6873727440834045, 'learning_rate': 0.00016, 'epoch': 0.9025069637883009, 'step': 81}, {'loss': 1.4337, 'grad_norm': 0.7600857019424438, 'learning_rate': 0.00015942857142857143, 'epoch': 0.9136490250696379, 'step': 82}, {'loss': 1.5755, 'grad_norm': 0.7032055854797363, 'learning_rate': 0.00015885714285714285, 'epoch': 0.924791086350975, 'step': 83}, {'loss': 1.2845, 'grad_norm': 0.6257392764091492, 'learning_rate': 0.0001582857142857143, 'epoch': 0.935933147632312, 'step': 84}, {'loss': 1.2835, 'grad_norm': 0.6865972280502319, 'learning_rate': 0.00015771428571428571, 'epoch': 0.947075208913649, 'step': 85}, {'loss': 1.4523, 'grad_norm': 0.7111278772354126, 'learning_rate': 0.00015714285714285716, 'epoch': 0.958217270194986, 'step': 86}, {'loss': 1.317, 'grad_norm': 0.7462661266326904, 'learning_rate': 0.00015657142857142858, 'epoch': 0.9693593314763231, 'step': 87}, {'loss': 1.365, 'grad_norm': 0.7058709859848022, 'learning_rate': 0.00015600000000000002, 'epoch': 0.9805013927576601, 'step': 88}, {'loss': 1.4684, 'grad_norm': 0.7146241664886475, 'learning_rate': 0.00015542857142857144, 'epoch': 0.9916434540389972, 'step': 89}, {'loss': 1.4619, 'grad_norm': 1.2372450828552246, 'learning_rate': 0.00015485714285714286, 'epoch': 1.0, 'step': 90}, {'loss': 1.2912, 'grad_norm': 0.6568215489387512, 'learning_rate': 0.0001542857142857143, 'epoch': 1.011142061281337, 'step': 91}, {'loss': 1.1859, 'grad_norm': 0.7042023539543152, 'learning_rate': 0.00015371428571428573, 'epoch': 1.0222841225626742, 'step': 92}, {'loss': 1.3262, 'grad_norm': 0.779255747795105, 'learning_rate': 0.00015314285714285714, 'epoch': 1.033426183844011, 'step': 93}, {'loss': 1.3166, 'grad_norm': 0.6254307627677917, 'learning_rate': 0.0001525714285714286, 'epoch': 1.0445682451253482, 'step': 94}, {'loss': 1.1872, 'grad_norm': 0.7047763466835022, 'learning_rate': 0.000152, 'epoch': 1.0557103064066853, 'step': 95}, {'loss': 1.3337, 'grad_norm': 0.7968904376029968, 'learning_rate': 0.00015142857142857143, 'epoch': 1.0668523676880224, 'step': 96}, {'loss': 1.3848, 'grad_norm': 0.8443092703819275, 'learning_rate': 0.00015085714285714287, 'epoch': 1.0779944289693593, 'step': 97}, {'loss': 1.2443, 'grad_norm': 0.7774724960327148, 'learning_rate': 0.0001502857142857143, 'epoch': 1.0891364902506964, 'step': 98}, {'loss': 1.3025, 'grad_norm': 0.6593716144561768, 'learning_rate': 0.0001497142857142857, 'epoch': 1.1002785515320335, 'step': 99}, {'loss': 1.2306, 'grad_norm': 0.7560638189315796, 'learning_rate': 0.00014914285714285713, 'epoch': 1.1114206128133706, 'step': 100}, {'loss': 1.2326, 'grad_norm': 0.7695331573486328, 'learning_rate': 0.00014857142857142857, 'epoch': 1.1225626740947074, 'step': 101}, {'loss': 1.3382, 'grad_norm': 0.725005030632019, 'learning_rate': 0.000148, 'epoch': 1.1337047353760445, 'step': 102}, {'loss': 1.3802, 'grad_norm': 0.7503019571304321, 'learning_rate': 0.00014742857142857144, 'epoch': 1.1448467966573816, 'step': 103}, {'loss': 1.1997, 'grad_norm': 0.7625697255134583, 'learning_rate': 0.00014685714285714288, 'epoch': 1.1559888579387188, 'step': 104}, {'loss': 1.2539, 'grad_norm': 0.8274160027503967, 'learning_rate': 0.0001462857142857143, 'epoch': 1.1671309192200556, 'step': 105}, {'loss': 1.5612, 'grad_norm': 0.8329058289527893, 'learning_rate': 0.00014571428571428572, 'epoch': 1.1782729805013927, 'step': 106}, {'loss': 1.1631, 'grad_norm': 0.7633733153343201, 'learning_rate': 0.00014514285714285717, 'epoch': 1.1894150417827298, 'step': 107}, {'loss': 1.1787, 'grad_norm': 0.8143183588981628, 'learning_rate': 0.00014457142857142859, 'epoch': 1.200557103064067, 'step': 108}, {'loss': 1.2205, 'grad_norm': 0.7023305296897888, 'learning_rate': 0.000144, 'epoch': 1.2116991643454038, 'step': 109}, {'loss': 1.2491, 'grad_norm': 0.7861520051956177, 'learning_rate': 0.00014342857142857145, 'epoch': 1.222841225626741, 'step': 110}, {'loss': 1.281, 'grad_norm': 0.7357292175292969, 'learning_rate': 0.00014285714285714287, 'epoch': 1.233983286908078, 'step': 111}, {'loss': 1.1913, 'grad_norm': 0.8698148131370544, 'learning_rate': 0.00014228571428571429, 'epoch': 1.2451253481894151, 'step': 112}, {'loss': 1.2318, 'grad_norm': 0.7782939076423645, 'learning_rate': 0.0001417142857142857, 'epoch': 1.2562674094707522, 'step': 113}, {'loss': 1.4442, 'grad_norm': 0.8289813995361328, 'learning_rate': 0.00014114285714285715, 'epoch': 1.267409470752089, 'step': 114}, {'loss': 1.2388, 'grad_norm': 0.90700763463974, 'learning_rate': 0.00014057142857142857, 'epoch': 1.2785515320334262, 'step': 115}, {'loss': 1.1815, 'grad_norm': 0.8196613192558289, 'learning_rate': 0.00014, 'epoch': 1.2896935933147633, 'step': 116}, {'loss': 1.177, 'grad_norm': 0.8808489441871643, 'learning_rate': 0.00013942857142857143, 'epoch': 1.3008356545961002, 'step': 117}, {'loss': 0.9639, 'grad_norm': 0.7351913452148438, 'learning_rate': 0.00013885714285714285, 'epoch': 1.3119777158774373, 'step': 118}, {'loss': 1.2745, 'grad_norm': 0.9481480121612549, 'learning_rate': 0.0001382857142857143, 'epoch': 1.3231197771587744, 'step': 119}, {'loss': 1.1701, 'grad_norm': 0.8162672519683838, 'learning_rate': 0.00013771428571428572, 'epoch': 1.3342618384401115, 'step': 120}, {'loss': 1.1421, 'grad_norm': 0.8762123584747314, 'learning_rate': 0.00013714285714285716, 'epoch': 1.3454038997214486, 'step': 121}, {'loss': 1.2855, 'grad_norm': 0.8784916996955872, 'learning_rate': 0.00013657142857142858, 'epoch': 1.3565459610027855, 'step': 122}, {'loss': 1.1857, 'grad_norm': 0.8617346286773682, 'learning_rate': 0.00013600000000000003, 'epoch': 1.3676880222841226, 'step': 123}, {'loss': 1.2371, 'grad_norm': 0.8123865127563477, 'learning_rate': 0.00013542857142857144, 'epoch': 1.3788300835654597, 'step': 124}, {'loss': 1.3232, 'grad_norm': 0.9031849503517151, 'learning_rate': 0.00013485714285714286, 'epoch': 1.3899721448467965, 'step': 125}, {'loss': 1.0672, 'grad_norm': 0.8493873476982117, 'learning_rate': 0.00013428571428571428, 'epoch': 1.4011142061281336, 'step': 126}, {'loss': 1.2483, 'grad_norm': 0.9660828113555908, 'learning_rate': 0.00013371428571428573, 'epoch': 1.4122562674094707, 'step': 127}, {'loss': 1.2925, 'grad_norm': 0.9241731762886047, 'learning_rate': 0.00013314285714285715, 'epoch': 1.4233983286908078, 'step': 128}, {'loss': 1.275, 'grad_norm': 0.7757856845855713, 'learning_rate': 0.00013257142857142856, 'epoch': 1.434540389972145, 'step': 129}, {'loss': 1.2155, 'grad_norm': 0.9286820888519287, 'learning_rate': 0.000132, 'epoch': 1.4456824512534818, 'step': 130}, {'loss': 1.3132, 'grad_norm': 0.9299753904342651, 'learning_rate': 0.00013142857142857143, 'epoch': 1.456824512534819, 'step': 131}, {'loss': 1.2481, 'grad_norm': 0.9572920203208923, 'learning_rate': 0.00013085714285714285, 'epoch': 1.467966573816156, 'step': 132}, {'loss': 1.2592, 'grad_norm': 0.888094961643219, 'learning_rate': 0.0001302857142857143, 'epoch': 1.479108635097493, 'step': 133}, {'loss': 1.1548, 'grad_norm': 0.9357700347900391, 'learning_rate': 0.0001297142857142857, 'epoch': 1.49025069637883, 'step': 134}, {'loss': 1.3797, 'grad_norm': 0.9513576030731201, 'learning_rate': 0.00012914285714285713, 'epoch': 1.501392757660167, 'step': 135}, {'loss': 1.343, 'grad_norm': 0.8834478259086609, 'learning_rate': 0.00012857142857142858, 'epoch': 1.5125348189415042, 'step': 136}, {'loss': 1.3297, 'grad_norm': 0.8490260243415833, 'learning_rate': 0.00012800000000000002, 'epoch': 1.5236768802228413, 'step': 137}, {'loss': 1.1588, 'grad_norm': 0.7613977193832397, 'learning_rate': 0.00012742857142857144, 'epoch': 1.5348189415041782, 'step': 138}, {'loss': 1.405, 'grad_norm': 0.8734720945358276, 'learning_rate': 0.00012685714285714286, 'epoch': 1.5459610027855153, 'step': 139}, {'loss': 0.8483, 'grad_norm': 0.948533296585083, 'learning_rate': 0.0001262857142857143, 'epoch': 1.5571030640668524, 'step': 140}, {'loss': 1.3055, 'grad_norm': 0.9260352253913879, 'learning_rate': 0.00012571428571428572, 'epoch': 1.5682451253481893, 'step': 141}, {'loss': 1.253, 'grad_norm': 0.8613424897193909, 'learning_rate': 0.00012514285714285714, 'epoch': 1.5793871866295266, 'step': 142}, {'loss': 1.3238, 'grad_norm': 0.9732642769813538, 'learning_rate': 0.0001245714285714286, 'epoch': 1.5905292479108635, 'step': 143}, {'loss': 1.3571, 'grad_norm': 0.9203901290893555, 'learning_rate': 0.000124, 'epoch': 1.6016713091922006, 'step': 144}, {'loss': 1.2181, 'grad_norm': 0.8901588916778564, 'learning_rate': 0.00012342857142857142, 'epoch': 1.6128133704735377, 'step': 145}, {'loss': 1.215, 'grad_norm': 0.8920788764953613, 'learning_rate': 0.00012285714285714287, 'epoch': 1.6239554317548746, 'step': 146}, {'loss': 1.2901, 'grad_norm': 0.9335202574729919, 'learning_rate': 0.0001222857142857143, 'epoch': 1.6350974930362117, 'step': 147}, {'loss': 1.2254, 'grad_norm': 1.0663762092590332, 'learning_rate': 0.00012171428571428572, 'epoch': 1.6462395543175488, 'step': 148}, {'loss': 1.3191, 'grad_norm': 0.9268290996551514, 'learning_rate': 0.00012114285714285715, 'epoch': 1.6573816155988856, 'step': 149}, {'loss': 1.2225, 'grad_norm': 0.8961048126220703, 'learning_rate': 0.00012057142857142858, 'epoch': 1.668523676880223, 'step': 150}, {'loss': 1.3906, 'grad_norm': 0.8739458322525024, 'learning_rate': 0.00012, 'epoch': 1.6796657381615598, 'step': 151}, {'loss': 1.2073, 'grad_norm': 0.8624740839004517, 'learning_rate': 0.00011942857142857145, 'epoch': 1.690807799442897, 'step': 152}, {'loss': 1.469, 'grad_norm': 0.9717026352882385, 'learning_rate': 0.00011885714285714287, 'epoch': 1.701949860724234, 'step': 153}, {'loss': 1.0713, 'grad_norm': 0.8592442274093628, 'learning_rate': 0.00011828571428571429, 'epoch': 1.713091922005571, 'step': 154}, {'loss': 1.1602, 'grad_norm': 0.9138263463973999, 'learning_rate': 0.0001177142857142857, 'epoch': 1.724233983286908, 'step': 155}, {'loss': 1.1958, 'grad_norm': 0.9719564914703369, 'learning_rate': 0.00011714285714285715, 'epoch': 1.7353760445682451, 'step': 156}, {'loss': 1.2154, 'grad_norm': 0.9177144169807434, 'learning_rate': 0.00011657142857142858, 'epoch': 1.7465181058495822, 'step': 157}, {'loss': 1.3155, 'grad_norm': 0.8689266443252563, 'learning_rate': 0.000116, 'epoch': 1.7576601671309193, 'step': 158}, {'loss': 1.0051, 'grad_norm': 0.7614632844924927, 'learning_rate': 0.00011542857142857145, 'epoch': 1.7688022284122562, 'step': 159}, {'loss': 1.1438, 'grad_norm': 0.8015786409378052, 'learning_rate': 0.00011485714285714286, 'epoch': 1.7799442896935933, 'step': 160}, {'loss': 1.2286, 'grad_norm': 0.9030166864395142, 'learning_rate': 0.00011428571428571428, 'epoch': 1.7910863509749304, 'step': 161}, {'loss': 1.2552, 'grad_norm': 0.9782418012619019, 'learning_rate': 0.00011371428571428573, 'epoch': 1.8022284122562673, 'step': 162}, {'loss': 1.1076, 'grad_norm': 0.79245525598526, 'learning_rate': 0.00011314285714285715, 'epoch': 1.8133704735376046, 'step': 163}, {'loss': 1.1676, 'grad_norm': 0.9273942112922668, 'learning_rate': 0.00011257142857142857, 'epoch': 1.8245125348189415, 'step': 164}, {'loss': 1.1326, 'grad_norm': 0.8613929748535156, 'learning_rate': 0.00011200000000000001, 'epoch': 1.8356545961002786, 'step': 165}, {'loss': 1.427, 'grad_norm': 0.9744288921356201, 'learning_rate': 0.00011142857142857144, 'epoch': 1.8467966573816157, 'step': 166}, {'loss': 1.184, 'grad_norm': 1.0441129207611084, 'learning_rate': 0.00011085714285714286, 'epoch': 1.8579387186629526, 'step': 167}, {'loss': 1.2386, 'grad_norm': 0.8676637411117554, 'learning_rate': 0.00011028571428571428, 'epoch': 1.8690807799442897, 'step': 168}, {'loss': 1.2778, 'grad_norm': 0.8851742744445801, 'learning_rate': 0.00010971428571428573, 'epoch': 1.8802228412256268, 'step': 169}, {'loss': 1.1401, 'grad_norm': 0.8548437356948853, 'learning_rate': 0.00010914285714285715, 'epoch': 1.8913649025069637, 'step': 170}, {'loss': 1.2466, 'grad_norm': 0.9864009618759155, 'learning_rate': 0.00010857142857142856, 'epoch': 1.902506963788301, 'step': 171}, {'loss': 1.2555, 'grad_norm': 0.9447543621063232, 'learning_rate': 0.00010800000000000001, 'epoch': 1.9136490250696379, 'step': 172}, {'loss': 1.2885, 'grad_norm': 0.9939294457435608, 'learning_rate': 0.00010742857142857143, 'epoch': 1.924791086350975, 'step': 173}, {'loss': 1.3608, 'grad_norm': 0.9055414795875549, 'learning_rate': 0.00010685714285714286, 'epoch': 1.935933147632312, 'step': 174}, {'loss': 1.0456, 'grad_norm': 1.069712519645691, 'learning_rate': 0.0001062857142857143, 'epoch': 1.947075208913649, 'step': 175}, {'loss': 1.2962, 'grad_norm': 1.121871829032898, 'learning_rate': 0.00010571428571428572, 'epoch': 1.958217270194986, 'step': 176}, {'loss': 1.3867, 'grad_norm': 1.0578590631484985, 'learning_rate': 0.00010514285714285714, 'epoch': 1.9693593314763231, 'step': 177}, {'loss': 1.06, 'grad_norm': 0.9466469287872314, 'learning_rate': 0.00010457142857142859, 'epoch': 1.98050139275766, 'step': 178}, {'loss': 1.1834, 'grad_norm': 0.9249342679977417, 'learning_rate': 0.00010400000000000001, 'epoch': 1.9916434540389973, 'step': 179}, {'loss': 1.2204, 'grad_norm': 1.202898621559143, 'learning_rate': 0.00010342857142857143, 'epoch': 2.0, 'step': 180}, {'loss': 0.8899, 'grad_norm': 0.8808924555778503, 'learning_rate': 0.00010285714285714286, 'epoch': 2.011142061281337, 'step': 181}, {'loss': 0.9769, 'grad_norm': 0.8589332699775696, 'learning_rate': 0.00010228571428571429, 'epoch': 2.022284122562674, 'step': 182}, {'loss': 0.9322, 'grad_norm': 0.8961280584335327, 'learning_rate': 0.00010171428571428572, 'epoch': 2.033426183844011, 'step': 183}, {'loss': 0.9763, 'grad_norm': 0.8732023239135742, 'learning_rate': 0.00010114285714285714, 'epoch': 2.0445682451253484, 'step': 184}, {'loss': 1.0586, 'grad_norm': 0.8978052735328674, 'learning_rate': 0.00010057142857142859, 'epoch': 2.0557103064066853, 'step': 185}, {'loss': 1.1251, 'grad_norm': 1.0404366254806519, 'learning_rate': 0.0001, 'epoch': 2.066852367688022, 'step': 186}, {'loss': 0.8799, 'grad_norm': 0.9223052859306335, 'learning_rate': 9.942857142857144e-05, 'epoch': 2.0779944289693595, 'step': 187}, {'loss': 0.9653, 'grad_norm': 0.9468458890914917, 'learning_rate': 9.885714285714286e-05, 'epoch': 2.0891364902506964, 'step': 188}, {'loss': 0.9322, 'grad_norm': 1.0293457508087158, 'learning_rate': 9.828571428571429e-05, 'epoch': 2.1002785515320332, 'step': 189}, {'loss': 1.251, 'grad_norm': 1.1362812519073486, 'learning_rate': 9.771428571428572e-05, 'epoch': 2.1114206128133706, 'step': 190}, {'loss': 1.0038, 'grad_norm': 1.0859663486480713, 'learning_rate': 9.714285714285715e-05, 'epoch': 2.1225626740947074, 'step': 191}, {'loss': 0.9639, 'grad_norm': 1.163569450378418, 'learning_rate': 9.657142857142858e-05, 'epoch': 2.1337047353760448, 'step': 192}, {'loss': 0.9213, 'grad_norm': 1.1565107107162476, 'learning_rate': 9.6e-05, 'epoch': 2.1448467966573816, 'step': 193}, {'loss': 1.1183, 'grad_norm': 1.1428498029708862, 'learning_rate': 9.542857142857143e-05, 'epoch': 2.1559888579387185, 'step': 194}, {'loss': 0.8748, 'grad_norm': 1.0950638055801392, 'learning_rate': 9.485714285714287e-05, 'epoch': 2.167130919220056, 'step': 195}, {'loss': 0.8508, 'grad_norm': 1.2114038467407227, 'learning_rate': 9.428571428571429e-05, 'epoch': 2.1782729805013927, 'step': 196}, {'loss': 0.9401, 'grad_norm': 1.3466984033584595, 'learning_rate': 9.371428571428572e-05, 'epoch': 2.1894150417827296, 'step': 197}, {'loss': 1.0095, 'grad_norm': 1.1176079511642456, 'learning_rate': 9.314285714285715e-05, 'epoch': 2.200557103064067, 'step': 198}, {'loss': 0.7663, 'grad_norm': 1.0043964385986328, 'learning_rate': 9.257142857142858e-05, 'epoch': 2.211699164345404, 'step': 199}, {'loss': 0.9264, 'grad_norm': 1.059462308883667, 'learning_rate': 9.200000000000001e-05, 'epoch': 2.222841225626741, 'step': 200}, {'loss': 0.9472, 'grad_norm': 1.1698787212371826, 'learning_rate': 9.142857142857143e-05, 'epoch': 2.233983286908078, 'step': 201}, {'loss': 0.9563, 'grad_norm': 1.1433671712875366, 'learning_rate': 9.085714285714286e-05, 'epoch': 2.245125348189415, 'step': 202}, {'loss': 0.9413, 'grad_norm': 1.1768677234649658, 'learning_rate': 9.028571428571428e-05, 'epoch': 2.256267409470752, 'step': 203}, {'loss': 1.0594, 'grad_norm': 1.063576340675354, 'learning_rate': 8.971428571428571e-05, 'epoch': 2.267409470752089, 'step': 204}, {'loss': 1.0186, 'grad_norm': 1.1832002401351929, 'learning_rate': 8.914285714285715e-05, 'epoch': 2.2785515320334264, 'step': 205}, {'loss': 1.0005, 'grad_norm': 1.0718501806259155, 'learning_rate': 8.857142857142857e-05, 'epoch': 2.2896935933147633, 'step': 206}, {'loss': 0.9004, 'grad_norm': 1.2927566766738892, 'learning_rate': 8.800000000000001e-05, 'epoch': 2.3008356545961, 'step': 207}, {'loss': 0.8994, 'grad_norm': 1.0507185459136963, 'learning_rate': 8.742857142857144e-05, 'epoch': 2.3119777158774375, 'step': 208}, {'loss': 1.0241, 'grad_norm': 1.1018463373184204, 'learning_rate': 8.685714285714286e-05, 'epoch': 2.3231197771587744, 'step': 209}, {'loss': 0.7902, 'grad_norm': 1.1322603225708008, 'learning_rate': 8.62857142857143e-05, 'epoch': 2.3342618384401113, 'step': 210}, {'loss': 1.0798, 'grad_norm': 1.1255419254302979, 'learning_rate': 8.571428571428571e-05, 'epoch': 2.3454038997214486, 'step': 211}, {'loss': 1.1961, 'grad_norm': 1.0970244407653809, 'learning_rate': 8.514285714285714e-05, 'epoch': 2.3565459610027855, 'step': 212}, {'loss': 1.0123, 'grad_norm': 1.1958916187286377, 'learning_rate': 8.457142857142858e-05, 'epoch': 2.3676880222841223, 'step': 213}, {'loss': 0.9768, 'grad_norm': 1.306734561920166, 'learning_rate': 8.4e-05, 'epoch': 2.3788300835654597, 'step': 214}, {'loss': 0.993, 'grad_norm': 1.1251142024993896, 'learning_rate': 8.342857142857143e-05, 'epoch': 2.3899721448467965, 'step': 215}, {'loss': 0.9679, 'grad_norm': 1.066713571548462, 'learning_rate': 8.285714285714287e-05, 'epoch': 2.401114206128134, 'step': 216}, {'loss': 1.0051, 'grad_norm': 1.1377936601638794, 'learning_rate': 8.228571428571429e-05, 'epoch': 2.4122562674094707, 'step': 217}, {'loss': 0.8694, 'grad_norm': 1.018409013748169, 'learning_rate': 8.171428571428572e-05, 'epoch': 2.4233983286908076, 'step': 218}, {'loss': 0.9875, 'grad_norm': 1.0899251699447632, 'learning_rate': 8.114285714285714e-05, 'epoch': 2.434540389972145, 'step': 219}, {'loss': 0.9701, 'grad_norm': 1.0866490602493286, 'learning_rate': 8.057142857142857e-05, 'epoch': 2.445682451253482, 'step': 220}, {'loss': 0.8769, 'grad_norm': 1.2298146486282349, 'learning_rate': 8e-05, 'epoch': 2.456824512534819, 'step': 221}, {'loss': 1.0338, 'grad_norm': 1.2152152061462402, 'learning_rate': 7.942857142857143e-05, 'epoch': 2.467966573816156, 'step': 222}, {'loss': 1.0429, 'grad_norm': 1.152283787727356, 'learning_rate': 7.885714285714286e-05, 'epoch': 2.479108635097493, 'step': 223}, {'loss': 0.9586, 'grad_norm': 1.2934434413909912, 'learning_rate': 7.828571428571429e-05, 'epoch': 2.4902506963788302, 'step': 224}, {'loss': 0.9697, 'grad_norm': 1.257043719291687, 'learning_rate': 7.771428571428572e-05, 'epoch': 2.501392757660167, 'step': 225}, {'loss': 1.0197, 'grad_norm': 1.1968834400177002, 'learning_rate': 7.714285714285715e-05, 'epoch': 2.5125348189415044, 'step': 226}, {'loss': 1.1543, 'grad_norm': 1.303082823753357, 'learning_rate': 7.657142857142857e-05, 'epoch': 2.5236768802228413, 'step': 227}, {'loss': 0.9567, 'grad_norm': 1.3333985805511475, 'learning_rate': 7.6e-05, 'epoch': 2.534818941504178, 'step': 228}, {'loss': 0.9957, 'grad_norm': 1.1520904302597046, 'learning_rate': 7.542857142857144e-05, 'epoch': 2.545961002785515, 'step': 229}, {'loss': 1.0322, 'grad_norm': 1.2680784463882446, 'learning_rate': 7.485714285714285e-05, 'epoch': 2.5571030640668524, 'step': 230}, {'loss': 0.8783, 'grad_norm': 1.1717265844345093, 'learning_rate': 7.428571428571429e-05, 'epoch': 2.5682451253481893, 'step': 231}, {'loss': 0.9746, 'grad_norm': 1.2173123359680176, 'learning_rate': 7.371428571428572e-05, 'epoch': 2.5793871866295266, 'step': 232}, {'loss': 0.9744, 'grad_norm': 1.336761474609375, 'learning_rate': 7.314285714285715e-05, 'epoch': 2.5905292479108635, 'step': 233}, {'loss': 0.972, 'grad_norm': 1.2372241020202637, 'learning_rate': 7.257142857142858e-05, 'epoch': 2.6016713091922004, 'step': 234}, {'loss': 1.0344, 'grad_norm': 1.2138657569885254, 'learning_rate': 7.2e-05, 'epoch': 2.6128133704735377, 'step': 235}, {'loss': 0.8757, 'grad_norm': 1.245123267173767, 'learning_rate': 7.142857142857143e-05, 'epoch': 2.6239554317548746, 'step': 236}, {'loss': 1.1645, 'grad_norm': 1.4150607585906982, 'learning_rate': 7.085714285714285e-05, 'epoch': 2.635097493036212, 'step': 237}, {'loss': 1.0551, 'grad_norm': 1.145595908164978, 'learning_rate': 7.028571428571428e-05, 'epoch': 2.6462395543175488, 'step': 238}, {'loss': 0.8572, 'grad_norm': 1.1699292659759521, 'learning_rate': 6.971428571428572e-05, 'epoch': 2.6573816155988856, 'step': 239}, {'loss': 1.1235, 'grad_norm': 1.3884214162826538, 'learning_rate': 6.914285714285715e-05, 'epoch': 2.668523676880223, 'step': 240}, {'loss': 0.9251, 'grad_norm': 1.1015559434890747, 'learning_rate': 6.857142857142858e-05, 'epoch': 2.67966573816156, 'step': 241}, {'loss': 0.9564, 'grad_norm': 1.2111090421676636, 'learning_rate': 6.800000000000001e-05, 'epoch': 2.690807799442897, 'step': 242}, {'loss': 0.9175, 'grad_norm': 1.2388737201690674, 'learning_rate': 6.742857142857143e-05, 'epoch': 2.701949860724234, 'step': 243}, {'loss': 1.1106, 'grad_norm': 1.3791526556015015, 'learning_rate': 6.685714285714286e-05, 'epoch': 2.713091922005571, 'step': 244}, {'loss': 0.8606, 'grad_norm': 1.3347069025039673, 'learning_rate': 6.628571428571428e-05, 'epoch': 2.724233983286908, 'step': 245}, {'loss': 1.0746, 'grad_norm': 1.2813656330108643, 'learning_rate': 6.571428571428571e-05, 'epoch': 2.735376044568245, 'step': 246}, {'loss': 1.1425, 'grad_norm': 1.33257257938385, 'learning_rate': 6.514285714285715e-05, 'epoch': 2.7465181058495824, 'step': 247}, {'loss': 0.9539, 'grad_norm': 1.3896063566207886, 'learning_rate': 6.457142857142856e-05, 'epoch': 2.7576601671309193, 'step': 248}, {'loss': 1.0554, 'grad_norm': 1.4907842874526978, 'learning_rate': 6.400000000000001e-05, 'epoch': 2.768802228412256, 'step': 249}, {'loss': 0.9892, 'grad_norm': 1.3507804870605469, 'learning_rate': 6.342857142857143e-05, 'epoch': 2.779944289693593, 'step': 250}, {'loss': 0.905, 'grad_norm': 1.522828221321106, 'learning_rate': 6.285714285714286e-05, 'epoch': 2.7910863509749304, 'step': 251}, {'loss': 0.8886, 'grad_norm': 1.2990481853485107, 'learning_rate': 6.22857142857143e-05, 'epoch': 2.8022284122562673, 'step': 252}, {'loss': 0.9957, 'grad_norm': 1.275412917137146, 'learning_rate': 6.171428571428571e-05, 'epoch': 2.8133704735376046, 'step': 253}, {'loss': 1.0448, 'grad_norm': 1.3157267570495605, 'learning_rate': 6.114285714285714e-05, 'epoch': 2.8245125348189415, 'step': 254}, {'loss': 1.012, 'grad_norm': 1.2819169759750366, 'learning_rate': 6.0571428571428576e-05, 'epoch': 2.8356545961002784, 'step': 255}, {'loss': 1.0521, 'grad_norm': 1.3115814924240112, 'learning_rate': 6e-05, 'epoch': 2.8467966573816157, 'step': 256}, {'loss': 1.0079, 'grad_norm': 1.4090064764022827, 'learning_rate': 5.9428571428571434e-05, 'epoch': 2.8579387186629526, 'step': 257}, {'loss': 1.0376, 'grad_norm': 1.2558966875076294, 'learning_rate': 5.885714285714285e-05, 'epoch': 2.86908077994429, 'step': 258}, {'loss': 0.9176, 'grad_norm': 1.3433914184570312, 'learning_rate': 5.828571428571429e-05, 'epoch': 2.8802228412256268, 'step': 259}, {'loss': 0.8058, 'grad_norm': 1.2147084474563599, 'learning_rate': 5.771428571428572e-05, 'epoch': 2.8913649025069637, 'step': 260}, {'loss': 0.9353, 'grad_norm': 1.2070540189743042, 'learning_rate': 5.714285714285714e-05, 'epoch': 2.902506963788301, 'step': 261}, {'loss': 0.887, 'grad_norm': 1.209380030632019, 'learning_rate': 5.6571428571428574e-05, 'epoch': 2.913649025069638, 'step': 262}, {'loss': 1.0138, 'grad_norm': 1.3335238695144653, 'learning_rate': 5.6000000000000006e-05, 'epoch': 2.924791086350975, 'step': 263}, {'loss': 0.8913, 'grad_norm': 1.2940541505813599, 'learning_rate': 5.542857142857143e-05, 'epoch': 2.935933147632312, 'step': 264}, {'loss': 0.9906, 'grad_norm': 1.232685923576355, 'learning_rate': 5.485714285714286e-05, 'epoch': 2.947075208913649, 'step': 265}, {'loss': 1.0771, 'grad_norm': 1.3194574117660522, 'learning_rate': 5.428571428571428e-05, 'epoch': 2.958217270194986, 'step': 266}, {'loss': 1.0372, 'grad_norm': 1.4183809757232666, 'learning_rate': 5.3714285714285714e-05, 'epoch': 2.969359331476323, 'step': 267}, {'loss': 0.8436, 'grad_norm': 1.2343769073486328, 'learning_rate': 5.314285714285715e-05, 'epoch': 2.98050139275766, 'step': 268}, {'loss': 0.9674, 'grad_norm': 1.3420333862304688, 'learning_rate': 5.257142857142857e-05, 'epoch': 2.9916434540389973, 'step': 269}, {'loss': 0.9891, 'grad_norm': 1.665450096130371, 'learning_rate': 5.2000000000000004e-05, 'epoch': 3.0, 'step': 270}, {'loss': 0.7949, 'grad_norm': 1.1158055067062378, 'learning_rate': 5.142857142857143e-05, 'epoch': 3.011142061281337, 'step': 271}, {'loss': 0.703, 'grad_norm': 1.1911174058914185, 'learning_rate': 5.085714285714286e-05, 'epoch': 3.022284122562674, 'step': 272}, {'loss': 0.6534, 'grad_norm': 1.2144885063171387, 'learning_rate': 5.028571428571429e-05, 'epoch': 3.033426183844011, 'step': 273}, {'loss': 1.0405, 'grad_norm': 1.3023581504821777, 'learning_rate': 4.971428571428572e-05, 'epoch': 3.0445682451253484, 'step': 274}, {'loss': 0.9281, 'grad_norm': 1.239483118057251, 'learning_rate': 4.9142857142857144e-05, 'epoch': 3.0557103064066853, 'step': 275}, {'loss': 0.7973, 'grad_norm': 1.2464518547058105, 'learning_rate': 4.8571428571428576e-05, 'epoch': 3.066852367688022, 'step': 276}, {'loss': 0.7582, 'grad_norm': 1.1818230152130127, 'learning_rate': 4.8e-05, 'epoch': 3.0779944289693595, 'step': 277}, {'loss': 0.8939, 'grad_norm': 1.4022009372711182, 'learning_rate': 4.742857142857143e-05, 'epoch': 3.0891364902506964, 'step': 278}, {'loss': 0.8468, 'grad_norm': 1.3683972358703613, 'learning_rate': 4.685714285714286e-05, 'epoch': 3.1002785515320332, 'step': 279}, {'loss': 0.6423, 'grad_norm': 1.344314694404602, 'learning_rate': 4.628571428571429e-05, 'epoch': 3.1114206128133706, 'step': 280}, {'loss': 0.6844, 'grad_norm': 1.5180082321166992, 'learning_rate': 4.5714285714285716e-05, 'epoch': 3.1225626740947074, 'step': 281}, {'loss': 0.715, 'grad_norm': 1.4964399337768555, 'learning_rate': 4.514285714285714e-05, 'epoch': 3.1337047353760448, 'step': 282}, {'loss': 0.7177, 'grad_norm': 1.4661939144134521, 'learning_rate': 4.4571428571428574e-05, 'epoch': 3.1448467966573816, 'step': 283}, {'loss': 0.8263, 'grad_norm': 1.6714519262313843, 'learning_rate': 4.4000000000000006e-05, 'epoch': 3.1559888579387185, 'step': 284}, {'loss': 0.756, 'grad_norm': 1.523932933807373, 'learning_rate': 4.342857142857143e-05, 'epoch': 3.167130919220056, 'step': 285}, {'loss': 0.7313, 'grad_norm': 1.369917869567871, 'learning_rate': 4.2857142857142856e-05, 'epoch': 3.1782729805013927, 'step': 286}, {'loss': 0.7744, 'grad_norm': 1.2942990064620972, 'learning_rate': 4.228571428571429e-05, 'epoch': 3.1894150417827296, 'step': 287}, {'loss': 0.8019, 'grad_norm': 1.5625442266464233, 'learning_rate': 4.1714285714285714e-05, 'epoch': 3.200557103064067, 'step': 288}, {'loss': 0.8196, 'grad_norm': 1.581431269645691, 'learning_rate': 4.1142857142857146e-05, 'epoch': 3.211699164345404, 'step': 289}, {'loss': 0.7567, 'grad_norm': 1.630020260810852, 'learning_rate': 4.057142857142857e-05, 'epoch': 3.222841225626741, 'step': 290}, {'loss': 0.9145, 'grad_norm': 1.4557689428329468, 'learning_rate': 4e-05, 'epoch': 3.233983286908078, 'step': 291}, {'loss': 0.624, 'grad_norm': 1.3693408966064453, 'learning_rate': 3.942857142857143e-05, 'epoch': 3.245125348189415, 'step': 292}, {'loss': 0.6516, 'grad_norm': 1.3954533338546753, 'learning_rate': 3.885714285714286e-05, 'epoch': 3.256267409470752, 'step': 293}, {'loss': 0.8338, 'grad_norm': 1.5013126134872437, 'learning_rate': 3.8285714285714286e-05, 'epoch': 3.267409470752089, 'step': 294}, {'loss': 0.7773, 'grad_norm': 1.51409912109375, 'learning_rate': 3.771428571428572e-05, 'epoch': 3.2785515320334264, 'step': 295}, {'loss': 0.7187, 'grad_norm': 1.4946297407150269, 'learning_rate': 3.7142857142857143e-05, 'epoch': 3.2896935933147633, 'step': 296}, {'loss': 0.7367, 'grad_norm': 1.3829625844955444, 'learning_rate': 3.6571428571428576e-05, 'epoch': 3.3008356545961, 'step': 297}, {'loss': 0.828, 'grad_norm': 1.5217595100402832, 'learning_rate': 3.6e-05, 'epoch': 3.3119777158774375, 'step': 298}, {'loss': 0.5638, 'grad_norm': 1.5061275959014893, 'learning_rate': 3.5428571428571426e-05, 'epoch': 3.3231197771587744, 'step': 299}, {'loss': 0.6291, 'grad_norm': 1.3517786264419556, 'learning_rate': 3.485714285714286e-05, 'epoch': 3.3342618384401113, 'step': 300}, {'loss': 0.8281, 'grad_norm': 1.5193148851394653, 'learning_rate': 3.428571428571429e-05, 'epoch': 3.3454038997214486, 'step': 301}, {'loss': 0.7314, 'grad_norm': 1.382870078086853, 'learning_rate': 3.3714285714285716e-05, 'epoch': 3.3565459610027855, 'step': 302}, {'loss': 0.7978, 'grad_norm': 1.4845515489578247, 'learning_rate': 3.314285714285714e-05, 'epoch': 3.3676880222841223, 'step': 303}, {'loss': 0.8092, 'grad_norm': 1.647055745124817, 'learning_rate': 3.257142857142857e-05, 'epoch': 3.3788300835654597, 'step': 304}, {'loss': 0.8202, 'grad_norm': 1.4277738332748413, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.3899721448467965, 'step': 305}, {'loss': 0.8005, 'grad_norm': 1.513954520225525, 'learning_rate': 3.142857142857143e-05, 'epoch': 3.401114206128134, 'step': 306}, {'loss': 0.7753, 'grad_norm': 1.5084102153778076, 'learning_rate': 3.0857142857142856e-05, 'epoch': 3.4122562674094707, 'step': 307}, {'loss': 0.7932, 'grad_norm': 1.5457192659378052, 'learning_rate': 3.0285714285714288e-05, 'epoch': 3.4233983286908076, 'step': 308}, {'loss': 0.7702, 'grad_norm': 1.601913332939148, 'learning_rate': 2.9714285714285717e-05, 'epoch': 3.434540389972145, 'step': 309}, {'loss': 0.7027, 'grad_norm': 1.3418031930923462, 'learning_rate': 2.9142857142857146e-05, 'epoch': 3.445682451253482, 'step': 310}, {'loss': 0.8293, 'grad_norm': 1.41641366481781, 'learning_rate': 2.857142857142857e-05, 'epoch': 3.456824512534819, 'step': 311}, {'loss': 0.9291, 'grad_norm': 1.556778907775879, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.467966573816156, 'step': 312}, {'loss': 0.8353, 'grad_norm': 1.5832990407943726, 'learning_rate': 2.742857142857143e-05, 'epoch': 3.479108635097493, 'step': 313}, {'loss': 0.6817, 'grad_norm': 1.4632056951522827, 'learning_rate': 2.6857142857142857e-05, 'epoch': 3.4902506963788302, 'step': 314}, {'loss': 0.6554, 'grad_norm': 1.501023769378662, 'learning_rate': 2.6285714285714286e-05, 'epoch': 3.501392757660167, 'step': 315}, {'loss': 0.5801, 'grad_norm': 1.35213041305542, 'learning_rate': 2.5714285714285714e-05, 'epoch': 3.5125348189415044, 'step': 316}, {'loss': 0.6792, 'grad_norm': 1.6650853157043457, 'learning_rate': 2.5142857142857147e-05, 'epoch': 3.5236768802228413, 'step': 317}, {'loss': 0.7687, 'grad_norm': 1.4169431924819946, 'learning_rate': 2.4571428571428572e-05, 'epoch': 3.534818941504178, 'step': 318}, {'loss': 0.7423, 'grad_norm': 1.392200231552124, 'learning_rate': 2.4e-05, 'epoch': 3.545961002785515, 'step': 319}, {'loss': 0.7152, 'grad_norm': 1.57988440990448, 'learning_rate': 2.342857142857143e-05, 'epoch': 3.5571030640668524, 'step': 320}, {'loss': 0.7334, 'grad_norm': 1.5222669839859009, 'learning_rate': 2.2857142857142858e-05, 'epoch': 3.5682451253481893, 'step': 321}, {'loss': 0.6905, 'grad_norm': 1.4425209760665894, 'learning_rate': 2.2285714285714287e-05, 'epoch': 3.5793871866295266, 'step': 322}, {'loss': 0.7625, 'grad_norm': 1.5328941345214844, 'learning_rate': 2.1714285714285715e-05, 'epoch': 3.5905292479108635, 'step': 323}, {'loss': 0.6704, 'grad_norm': 1.6462445259094238, 'learning_rate': 2.1142857142857144e-05, 'epoch': 3.6016713091922004, 'step': 324}, {'loss': 0.7214, 'grad_norm': 1.4737701416015625, 'learning_rate': 2.0571428571428573e-05, 'epoch': 3.6128133704735377, 'step': 325}, {'loss': 0.5799, 'grad_norm': 1.5693087577819824, 'learning_rate': 2e-05, 'epoch': 3.6239554317548746, 'step': 326}, {'loss': 0.8156, 'grad_norm': 1.6018599271774292, 'learning_rate': 1.942857142857143e-05, 'epoch': 3.635097493036212, 'step': 327}, {'loss': 0.7486, 'grad_norm': 1.5538898706436157, 'learning_rate': 1.885714285714286e-05, 'epoch': 3.6462395543175488, 'step': 328}, {'loss': 0.8306, 'grad_norm': 1.6485199928283691, 'learning_rate': 1.8285714285714288e-05, 'epoch': 3.6573816155988856, 'step': 329}, {'loss': 0.7502, 'grad_norm': 1.546593189239502, 'learning_rate': 1.7714285714285713e-05, 'epoch': 3.668523676880223, 'step': 330}, {'loss': 0.7382, 'grad_norm': 1.8148219585418701, 'learning_rate': 1.7142857142857145e-05, 'epoch': 3.67966573816156, 'step': 331}, {'loss': 0.6511, 'grad_norm': 1.5393356084823608, 'learning_rate': 1.657142857142857e-05, 'epoch': 3.690807799442897, 'step': 332}, {'loss': 0.7525, 'grad_norm': 1.5464411973953247, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.701949860724234, 'step': 333}, {'loss': 0.8704, 'grad_norm': 1.8449902534484863, 'learning_rate': 1.5428571428571428e-05, 'epoch': 3.713091922005571, 'step': 334}, {'loss': 0.7314, 'grad_norm': 1.5774720907211304, 'learning_rate': 1.4857142857142858e-05, 'epoch': 3.724233983286908, 'step': 335}, {'loss': 0.7432, 'grad_norm': 1.547113299369812, 'learning_rate': 1.4285714285714285e-05, 'epoch': 3.735376044568245, 'step': 336}, {'loss': 0.8404, 'grad_norm': 1.6057202816009521, 'learning_rate': 1.3714285714285716e-05, 'epoch': 3.7465181058495824, 'step': 337}, {'loss': 0.7436, 'grad_norm': 1.4626140594482422, 'learning_rate': 1.3142857142857143e-05, 'epoch': 3.7576601671309193, 'step': 338}, {'loss': 0.6857, 'grad_norm': 1.593131422996521, 'learning_rate': 1.2571428571428573e-05, 'epoch': 3.768802228412256, 'step': 339}, {'loss': 0.8553, 'grad_norm': 1.7452821731567383, 'learning_rate': 1.2e-05, 'epoch': 3.779944289693593, 'step': 340}, {'loss': 0.756, 'grad_norm': 1.5740083456039429, 'learning_rate': 1.1428571428571429e-05, 'epoch': 3.7910863509749304, 'step': 341}, {'loss': 0.7933, 'grad_norm': 1.4259893894195557, 'learning_rate': 1.0857142857142858e-05, 'epoch': 3.8022284122562673, 'step': 342}, {'loss': 0.7299, 'grad_norm': 1.6566134691238403, 'learning_rate': 1.0285714285714286e-05, 'epoch': 3.8133704735376046, 'step': 343}, {'loss': 0.8146, 'grad_norm': 1.5481970310211182, 'learning_rate': 9.714285714285715e-06, 'epoch': 3.8245125348189415, 'step': 344}, {'loss': 0.57, 'grad_norm': 1.5773943662643433, 'learning_rate': 9.142857142857144e-06, 'epoch': 3.8356545961002784, 'step': 345}, {'loss': 0.8398, 'grad_norm': 1.491611123085022, 'learning_rate': 8.571428571428573e-06, 'epoch': 3.8467966573816157, 'step': 346}, {'loss': 0.8485, 'grad_norm': 1.546915888786316, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.8579387186629526, 'step': 347}, {'loss': 0.7292, 'grad_norm': 1.5270289182662964, 'learning_rate': 7.428571428571429e-06, 'epoch': 3.86908077994429, 'step': 348}, {'loss': 0.8672, 'grad_norm': 1.5310587882995605, 'learning_rate': 6.857142857142858e-06, 'epoch': 3.8802228412256268, 'step': 349}, {'loss': 0.7208, 'grad_norm': 1.5633641481399536, 'learning_rate': 6.285714285714287e-06, 'epoch': 3.8913649025069637, 'step': 350}, {'loss': 0.7705, 'grad_norm': 1.4406814575195312, 'learning_rate': 5.7142857142857145e-06, 'epoch': 3.902506963788301, 'step': 351}, {'loss': 0.9596, 'grad_norm': 1.8103057146072388, 'learning_rate': 5.142857142857143e-06, 'epoch': 3.913649025069638, 'step': 352}, {'loss': 0.7703, 'grad_norm': 1.578397512435913, 'learning_rate': 4.571428571428572e-06, 'epoch': 3.924791086350975, 'step': 353}, {'loss': 0.7313, 'grad_norm': 1.5762372016906738, 'learning_rate': 4.000000000000001e-06, 'epoch': 3.935933147632312, 'step': 354}, {'loss': 0.7547, 'grad_norm': 1.6515579223632812, 'learning_rate': 3.428571428571429e-06, 'epoch': 3.947075208913649, 'step': 355}, {'loss': 0.7344, 'grad_norm': 1.7203983068466187, 'learning_rate': 2.8571428571428573e-06, 'epoch': 3.958217270194986, 'step': 356}, {'loss': 0.7946, 'grad_norm': 1.6125593185424805, 'learning_rate': 2.285714285714286e-06, 'epoch': 3.969359331476323, 'step': 357}, {'loss': 0.7188, 'grad_norm': 1.5808465480804443, 'learning_rate': 1.7142857142857145e-06, 'epoch': 3.98050139275766, 'step': 358}, {'loss': 0.7442, 'grad_norm': 1.4787790775299072, 'learning_rate': 1.142857142857143e-06, 'epoch': 3.9916434540389973, 'step': 359}, {'loss': 0.6485, 'grad_norm': 2.0840837955474854, 'learning_rate': 5.714285714285715e-07, 'epoch': 4.0, 'step': 360}, {'train_runtime': 507.965, 'train_samples_per_second': 5.646, 'train_steps_per_second': 0.709, 'total_flos': 5395186352369664.0, 'train_loss': 1.177739558286137, 'epoch': 4.0, 'step': 360}]\n",
      "Final loss: 1.177739558286137\n",
      "Training steps: 360\n"
     ]
    }
   ],
   "source": [
    "train_history = trainer.state.log_history\n",
    "print(\"Training logs:\", train_history)\n",
    "\n",
    "# Extract specific metrics\n",
    "losses = [log['train_loss'] for log in train_history if 'train_loss' in log]\n",
    "learning_rates = [log['learning_rate'] for log in train_history if 'learning_rate' in log]\n",
    "\n",
    "print(f\"Final loss: {losses[-1] if losses else 'N/A'}\")\n",
    "print(f\"Training steps: {trainer.state.global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf8d5f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.6485\n",
      "Training steps: 360\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Training Summary:\n",
      "Initial loss: 3.2328\n",
      "Final loss: 0.6485\n",
      "Loss reduction: 2.5843\n",
      "Percent improvement: 79.94%\n",
      "Min loss: 0.5638 at step 299\n",
      "Max gradient norm: 2.0841\n",
      "Final gradient norm: 2.0841\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract metrics from training history\n",
    "losses = [log['loss'] for log in train_history if 'loss' in log and 'train_runtime' not in log]\n",
    "learning_rates = [log['learning_rate'] for log in train_history if 'learning_rate' in log and 'train_runtime' not in log]\n",
    "grad_norms = [log['grad_norm'] for log in train_history if 'grad_norm' in log and 'train_runtime' not in log]\n",
    "steps = [log['step'] for log in train_history if 'step' in log and 'train_runtime' not in log]\n",
    "\n",
    "print(f\"Final loss: {losses[-1] if losses else 'N/A'}\")\n",
    "print(f\"Training steps: {steps[-1] if steps else 'N/A'}\")\n",
    "\n",
    "# Create plots\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Training Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(steps, losses, 'b-', linewidth=2, marker='o', markersize=3)\n",
    "plt.title('Training Loss Over Steps')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "if len(losses) > 1:\n",
    "   z = np.polyfit(steps, losses, 1)\n",
    "   p = np.poly1d(z)\n",
    "   plt.plot(steps, p(steps), 'r--', alpha=0.7, label=f'Trend: {z[0]:.4f}x + {z[1]:.4f}')\n",
    "   plt.legend()\n",
    "\n",
    "# Plot 2: Learning Rate Schedule\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(steps, learning_rates, 'g-', linewidth=2, marker='s', markersize=3)\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "\n",
    "# Plot 3: Gradient Norm\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(steps, grad_norms, 'orange', linewidth=2, marker='^', markersize=3)\n",
    "plt.title('Gradient Norm Over Steps')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Gradient Norm')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary stats\n",
    "print(f\"\\n📊 Training Summary:\")\n",
    "print(f\"Initial loss: {losses[0]:.4f}\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Loss reduction: {losses[0] - losses[-1]:.4f}\")\n",
    "print(f\"Percent improvement: {((losses[0] - losses[-1]) / losses[0] * 100):.2f}%\")\n",
    "print(f\"Min loss: {min(losses):.4f} at step {steps[losses.index(min(losses))]}\")\n",
    "print(f\"Max gradient norm: {max(grad_norms):.4f}\")\n",
    "print(f\"Final gradient norm: {grad_norms[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e4f0952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Who is Eutropia?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Eutropia is the ruler of Eurythnia, a human nation located in the River Kingdoms, known for her love of books, scholarship, and her title as the \"Wise One\".<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Format your input using the same chat template\n",
    "def format_input(instruction):\n",
    "    return f\"\"\"<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "# Run inference\n",
    "instruction = \"Who is Eutropia?\"\n",
    "inputs = tokenizer(format_input(instruction), return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f6c698",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5490e19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /home/ubuntu/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Downloading safetensors index for unsloth/llama-3.2-3b-instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|██████████| 2/2 [00:51<00:00, 25.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Models saved!\n",
      "LoRA adapter: ./lora_model/\n",
      "Merged model: ./merged_model/\n"
     ]
    }
   ],
   "source": [
    "# Save LoRA adapter\n",
    "model.save_pretrained(\"lora_model\")\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "\n",
    "# Save merged model  \n",
    "model.save_pretrained_merged(\"merged_model\", tokenizer, save_method=\"merged_16bit\")\n",
    "\n",
    "print(\"✅ Models saved!\")\n",
    "print(\"LoRA adapter: ./lora_model/\")\n",
    "print(\"Merged model: ./merged_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6f0a33",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06650dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.2: Fast Llama patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    Quadro RTX 6000. Num GPUs = 1. Max memory: 23.461 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"lora_model\",  # Your saved model path\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c64374",
   "metadata": {},
   "source": [
    "## Save for ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d61d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 26.17 out of 45.08 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 60.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting llama model. Can use fast conversion = False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at ollama_model into f16 GGUF format.\n",
      "The output location will be /home/ubuntu/PF_GM_aid/fine-tune/ollama_model/unsloth.F16.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: ollama_model\n",
      "INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {3072, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 24\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting special token type pad to 128004\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/home/ubuntu/PF_GM_aid/fine-tune/ollama_model/unsloth.F16.gguf: n_tensors = 255, total_size = 6.4G\n",
      "Writing: 100%|██████████| 6.43G/6.43G [00:07<00:00, 816Mbyte/s] \n",
      "INFO:hf-to-gguf:Model successfully exported to /home/ubuntu/PF_GM_aid/fine-tune/ollama_model/unsloth.F16.gguf\n",
      "Unsloth: Conversion completed! Output location: /home/ubuntu/PF_GM_aid/fine-tune/ollama_model/unsloth.F16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
      "main: build = 5686 (e434e691)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/home/ubuntu/PF_GM_aid/fine-tune/ollama_model/unsloth.F16.gguf' to '/home/ubuntu/PF_GM_aid/fine-tune/ollama_model/unsloth.Q4_K_M.gguf' as Q4_K_M using 28 threads\n",
      "llama_model_loader: loaded meta data with 27 key-value pairs and 255 tensors from /home/ubuntu/PF_GM_aid/fine-tune/ollama_model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Ollama_Model\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 3.2B\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type  f16:  197 tensors\n",
      "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q6_K .. size =   751.50 MiB ->   308.23 MiB\n",
      "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "llama_model_quantize_impl: model size  =  6128.17 MB\n",
      "llama_model_quantize_impl: quant size  =  1918.35 MB\n",
      "\n",
      "main: quantize time = 26245.12 ms\n",
      "main:    total time = 26245.13 ms\n",
      "Unsloth: Conversion completed! Output location: /home/ubuntu/PF_GM_aid/fine-tune/ollama_model/unsloth.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\"ollama_model\", tokenizer, quantization_method=\"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7acd9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
